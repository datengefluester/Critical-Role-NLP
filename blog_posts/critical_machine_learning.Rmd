---
title: "Critical Machine Learning: Prediciting Critical Role Actors from Text"
always_allow_html: true
output:
  html_document: default
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_format = "all") })
---

```{r setup, include=FALSE}
# tell Rmarkdown to run in root folder of project
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

# where figures for rendering the document are saved
knitr::opts_chunk$set(
  fig.path = "markdown_figs/"
)


# packages
library(dplyr) # for data preparation
library(tidyr) # for data preparation
library(stringr) # for data preparation
library(tidytext) # for rearrange facet + bar charts
library(ggplot2) # for graphs
library(ggpubr) # for graphs next to each other
library(ggraph) # for the network graph
library(igraph) # for the network graph
library(kableExtra) # for table
library(purrr) # for pluck command
library(yardstick) # for confusion matrix
library(ggforce) # for drawing circles in the knn-graph
library(data.table) # for faster reading in data
```

```{r themes, include=FALSE}
# graph themes
source("./scripts/themes.R")
```

```{r data, echo = FALSE}
# ------  read in every data frame from the 'machine_learning' folder
path <- "./data/machine_learning/"
# folders
folders_path <- list.files(path = path,pattern="", full.names = TRUE)
folders_path <- str_replace(folders_path, pattern="//", replacement="/")

# extract files and import them
# !!!!!!!! final version: (i in length(folders_path))
# now just placeholders with folders containing data
for (i in c(1,2,3,5,6,7)){
  tmp <- folders_path[i]
  files <- list.files(path = paste(tmp), pattern =".csv")
  files_path <- paste(folders_path[i],"/", files, sep ="")
  file_name <- str_replace(files,".csv","")
  myfiles = lapply(files_path, fread)

  for (df in 1:length(myfiles)){
tmp <- myfiles[[df]] %>% as.data.frame()
assign(paste(file_name[df]),tmp)
  }}


# ------  accuracy for all models combined

accuracy_overview <- decision_acc %>% 
  bind_rows(.,knn_acc) %>% 
  bind_rows(.,naive_bayes_acc) %>% 
  bind_rows(.,regularized_acc) %>% 
  bind_rows(.,rf_acc) #%>% 
  #bind_rows(.,xgboost_acc) %>% 
  #bind_rows(.,xgb_tune_acc) %>% 
  #bind_rows(.,multinominal_acc)
  
#rm(decision_acc,knn_acc,naive_bayes_acc,regularized_acc,rf_acc,xgboost_acc,xgb_tune_acc,multinominal_acc)

# ------ time and turns by actor
actor_words_time_percent <- read.csv('./data/descriptive_analysis/actor_words_time.csv')
actor_words_time_percent <- actor_words_time_percent %>%
  select(1, 4, 10) %>%
  pivot_longer(
    cols = c("words_percent","turns_percent"),
    names_to = "variable",
    values_to = "percent"
  ) %>%
  mutate(
    variable = str_replace(variable, "words_percent", "Words"),
    variable = str_replace(variable, "turns_percent", "Turns")
  )

# ------ example data
example <- read.csv('./data/descriptive_analysis/example.csv')
example_ml <- read.csv("./data/machine_learning/example_ml/example_ml.csv")
example_ml <- example_ml %>% 
  select(actor_guest,text,time_in_sec,words_per_minute,arc,segment,combat) %>%
  mutate(segment= str_to_title(segment)) %>% 
  rename(Actor = actor_guest,
         Text = text,
        `Time in Sec` = time_in_sec,
       `Words per Minute` =  words_per_minute,
       Arc = arc,
       Segment = segment,
       Combat = combat) 

# ------ Cross Validation Graph
nrows <- 10
cross_validation <- expand.grid(y = 1:nrows, x = 1:nrows)

cross_validation <- cross_validation %>% 
  mutate(train_test = "Train Data") %>% 
  mutate(train_test = replace(train_test, x==y, "Test Data")) %>% 
  mutate(train_test = as.factor(train_test))

# ------ Example graph to visualize the algorithms
set.seed(1000)
Liam <- rep(c("Liam"), each = 10)
Liam_x <- rnorm(10,2,0.8)
Liam_y <- rnorm(10,5,0.7)
Liam_df <- data.frame(actor= Liam,
                      x = Liam_x,
                      y = Liam_y)
Matt <- rep(c("Matt"), each = 10)
Matt_x <- rnorm(10,5,0.5)
Matt_y <- rnorm(10,2,0.9)
Matt_df <- data.frame(actor= Matt,
                      x = Matt_x,
                      y = Matt_y)
graph <- rbind(Liam_df,Matt_df)
# add noise 
graph <- graph %>% 
  add_row(actor="Matt",x = 1.8, y = 4.6) %>% 
  add_row(actor="Matt",x = 2.2, y = 4.5) %>% 
  add_row(actor="Liam",x = 5, y = 3) %>% 
  add_row(actor="Liam",x = 5.2, y = 3.2) 


# clean up 
rm(df,files,file_name, files_path,folders_path,i,tmp, myfiles, path)
# clean up
rm(Liam,Liam_x,Liam_y,Matt, Matt_x, Matt_y, nrows, Matt_df, Liam_df)
```

# 1. Introduction

In the last [post](https://datengefluester.de/critical-graphs-shenanigans-with-critical-role-season-1-subtitles/) we looked at subtitles from season one of Critical Role. During the analysis we have already seen that some words could potentially tell us which text is spoken by which actor. So let's go on step further and examine whether we can indeed predict the speaker using more advanced methods. Predicting the label of a text (in our case the actor) is a common task in machine learning. It is used, for example, when deciding whether an email is spam or not. However, in contrast to spam filters, we have nine classes, instead of two, to choose from (the dungeon master, the players and 'guests' versus spam or no spam). In statistical jargon this is referred to as multiclass classification. Moreover, because of the improv elements of the show it is likely going to be harder to identify the speaker than deciding whether a message is spam. This is because players are likely use similar vocabulary throughout the show. With that in mind, let's explore different methods to see which one, if any, we can be use to confidentially match text to actors.

# 2. General Approach: Training & Testing

We have already seen that the subtitles cover 431 hours of game play over 114 episodes. This amounts to 280,000 turns, or classified samples, which we can use to train our algorithms. However, we cannot use all of the data to train our algorithms. Instead we have to split them into what is called training and testing data set. Why do we have to do this? Imagine we would have to take an exam, where we know all the questions before hand. Our most efficient approach would be to learn all the answers by heart. While this helps us to ace the test, it does not help us to answer questions, which we have not seen before. In other words, our knowledge does not generalize well to similar questions. The same is true for algorithms. Therefore we want to have a final exam which contains questions or texts the algorithm has not seen before to know how well it did. In machine learning jargon this is called the test data set. The test set allows us to compare different models against each other.

But how would an algorithm know which features belong to which actor? This is what the training data set is for. As the name suggests, we use it to train our algorithm. However, algorithms contain many gear wheels, with different combinations leading to different predictions leading to different predictions. Thus, we need to have way to tell which combination of these parts works best. We achieve this my mimicing mock exams using parts of the data. Similarly to the final exam, we can use these mock exams to find the best configuration of the algorithm. To simulate many different mock exams, we can split our training data into multiple sets. Afterwards we train the model multiple times with each of the set functioning as training and testing at different times. In the machine learning jargon, those sets are called folds and process of using them to tune algorithms is called cross-validation. This approach gives us a better indication of how well a specification of an algorithm works. A common practice and the one we will use is to use 10-fold to train the algorithms. This means that we use 90 percent of the data to tune our algorithm and test it on 10 percent of the data. We repeat the process 10 times to find the best specification. The image below pictures the process.

```{r cross_validation, echo = FALSE}
ggplot(cross_validation, aes(x = x, y = y, fill = train_test)) + 
  geom_tile(color = "#3B3B3B", size = 0.1, alpha = 0.7) +
  scale_x_continuous(expand = c(0, 0),
                     breaks = c(1,10),
                     labels = c("1"="First 10%",
                                "10"="Last 10%")) +
  scale_y_continuous(expand = c(0, 0), trans = 'reverse',
                     breaks = c(seq(0, 10, 1)),
                     labels = c("0" ="",
                                "1" = "Fold 1", 
                                "2" = "Fold 2",
                                "3" = "Fold 3",
                                "4" = "Fold 4",
                                "5" = "Fold 5",
                                "6" = "Fold 6",
                                "7" = "Fold 7",
                                "8" = "Fold 8",
                                "9" = "Fold 9",
                                "10" = "Fold 10")) +
   scale_fill_manual(
    values = c("#ffa600","#1b9e77")
  ) +
  labs(title="10-Fold Cross Validation",
       subtitle="One Block represents Ten Percent of the Training Data") +
  bar_chart_theme() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 5.5),
    legend.title = element_blank(),
    legend.key.height = unit(0.4, "line"),
    legend.key = element_blank(),
    legend.margin = margin(t = 0, unit = "cm"),
    axis.title.x=element_blank(),
    axis.ticks.x=element_blank(),
    axis.ticks.y=element_blank(),
    panel.grid.major.x = element_blank() ,
    )
# save
ggsave("./output/images/machine_learning/ml_cross_validation.jpg", width = 4, height = 3)
```

# 3. Unbalancedness, Down-sampling, Accuracy

As we have seen in the last post, not every actor was present for every episode. While Orion was only present for around 24 of the episode, Matt was there for all of them. Likewise, Matt spoke for much of the time compared to other members of the cast. We need to account for this imbalance when estimating our algorithms. As 28 percent of our texts belong to Matt, simply guessing him for every text would result in us being correct in a little more than one fourth cases. So every algorithm that we train has to beat this benchmark to be considered an improvement over simply guessing Matt all the time.

```{r time_vs_words, echo = FALSE}
actor_words_time_percent %>%
  filter(variable != "Words") %>% 
  mutate(
    actor_guest = reorder_within(actor_guest, -percent, variable)
  ) %>%
  ggplot(aes(x = reorder(as.factor(actor_guest), percent), y = percent, fill = variable)) +
  geom_bar(stat = "identity", fill = "#009E73") +
  coord_flip() +
  scale_y_continuous(expand = c(0, 0)) +
  facet_wrap(~variable, scales = "free") +
  scale_x_reordered() +
  labs(
    title = "Texts per Actor",
    subtitle = "In percent of the total number of texts",
    caption = "Source: Critical-Role-Subtitles, CritStats",
  ) +
  bar_chart_theme() +
  theme(
    strip.background = element_rect(fill = "#F0F0F0"),
    panel.grid.major.x = element_line(size = rel(0.4), color = "#656565"),
    axis.line.y = element_line(size = rel(0.4), color = "black"),
    axis.text.x = element_text(color = "black", vjust = 5, size = rel(0.5), alpha(2)),
    axis.text.y = element_text(hjust = 1, color = "black", size = rel(0.75), margin = margin(r = -2)),
    strip.text = element_text(size = 5, face = "bold"),
  )

# save graph
ggsave("./output/images/machine_learning/ml_time_words_per_actor.jpg", width = 4, height = 3)
```

Another consequence of our data being unbalanced is the fact that both the training and testing data set will contain more texts from Matt. Thus, our algorithms are likely to learn more features for the texts of Matt compared to other actors as this results in better prediction. However, this would also result in our model not really learning how to predict Ashley, Orion or the guests. I think predicting the text should include all actors. After all, that is kind of the idea of the analysis to see how speakers differ. So how would be achieve this?

One approach to combat imbalance of data is called down-sampling. The idea of down-sampling is to leave out some text of the speakers, who speak the most until every speaker has the same amount of text. This will helps us to identify text spoken by speaker, who speak less compared to others. However, this comes at the cost of giving them more weight to them than they should have given how a game of Dungeons and Dragons is played. After all, an episode of the show will almost certainly include more speaking from the dungeon master than the player as everything has to be described and NPCs have to be played out. Hence, if our goal would only be to achieve the best prediction we would not down-sample.

We could also do the opposite and increase the number of samples for the actor, who speak less (the process is called up-sampling). To do so we would duplicate text for those speakers. However, we would have to increase the number of turns for many actors significantly given the imbalance of the data. This in turn would increase the time to train the algorithms by a lot. As some of the models already take quite some time to train we down sample to save time (and money as computing time is expensive).

An additional benefit of down-sampling is the fact that we can use accuracy as our metric to compare different algorithms. What does accuracy mean and why do we need it? Accuracy measures the number of right predictions as a proportion of all predictions. Basically, it tells us how many texts we assigned the right speaker to. Although more advanced metrics for evaluating models are available, accuracy is the most intuitive. It will give us the easiest and more illustrative way of comparing the different models. Thus, we will look at accuracy as our metrics in the following.

Lastly, as we have seen in our previous analysis, we sometimes encounter that actors say the same thing simultenously. Although, this happens rarely and it's even more rare for the same two actors to say something at the same time multiple times through out the series. If we would consider these case as separate speakers (e.g. Laura and Travis: as one speaker), we would significantly increase the number of classes we would need to predict to around 110. This includes all of combinations of actors, who say something at the same time together during all the hours of game play. Some of which only happen once in the entire series. Thus, we omit cases where actors say something simultaneously from our analysis. Putting everything together, we end up with 34,500 samples, which we can use to train our algorithms.

# 4. Modeling the Data

So far we have to dealt with the fact that the original data included many speakers and an unequal amount of texts for each actor. Now, we can turn to the question on how to model the data. Recall, that our raw text data from the previous post looks as follows:

```{r example, echo = FALSE}
example %>%
  select(-Segment) %>% 
  kbl() %>%
  kable_styling()
```

We can include some additional information to ultimately turn the data into something like this:

```{r example_ml, echo = FALSE}
example_ml %>%
  kbl() %>%
  kable_styling()
```

Please note that we dropped the episode number from the data so the algorithm does not train on episode specific dialogue. We keep the arc to allow for some general changes in the story to be represented in the data.

So far we have not changed with the spoken text. Let's do that now. As our goal is to predict the speaker from the text, we first have to turn the text into a nicer format. The first step is to transform the text into separate smaller pieces called tokens. The tokens can be single words or characters or combinations of words (called n-grams). For example, the text 'I love you' said by a player can be separate using 2-grams as follows: 'I love' and 'love you' or four tokens (I, love and you).

Yet, most of the text may not help us identifying the player. For example, all players announce their action using some form of 'I' (I will, I am going to etc.). Likewise, many words like 'and' or 'the' do not add much unique meaning to a text. Thus, they are unlikely to help us identify a speaker. Similarly, the number of unique words over all the episodes is pretty big and not every random shout out by a cast member will help us differentiate between them. To help us decide, which words are likely to help our prediction, we can calculate the 'term frequency inverse document frequency'. The measure calculates how often a term is used, while adjusting for the number of texts it is used in. Calculating the number of each word or n-grams gives us an indication of how helpful certain these are for predicting the speaker.

In theory, we could use all the words up to whole sentences to predict a speaker. Moreover, we could let the algorithm decide how many words it would like to include for its predicting (thus treating the number of words or n-grams as another moving part). However, doing so would significantly increase the time to train our models. Hence we restrict our analysis to include the 300 most predictive words, 2-grams and 3-grams. In combination with extracting some text features, like the number of words in a text, this gives a total of 318 variables to be predict the speakers. Let's see if this is enough of any of the most used machine learning algorithms to predict the speaker successfully.

# 5. K-Nearest Neighbors

The first approach we can use to try to predict the speakers is called k-nearest neighbors. The approach is simple. When we have to classify a text, we take the k number of nearest texts and take the speaker, which is most present within these texts. This may a bit unituitive when it comes to text. So let's examine the duration a text was spoken over and the number of words it contain as a more illustrative example. The graph below plots some imaginary data for two speakers: Matt and Liam. When we have to assign a speaker to a new text (the black point in the graph below), we choose the speaker, which is the most present within the k nearest points. For point one, this means that the point is classified as 'Liam' as more points are from Liam than from Matt. For point two the opposite applies.

```{r knn_graph, echo = FALSE}
graph %>% 
  add_row(actor="New Point",x = 2, y = 4.5) %>% 
  add_row(actor="New Point",x = 5, y = 2) %>% 
  ggplot(aes(x,y,color=actor)) + 
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77","#ffa600", "black")) +
  scale_y_continuous(expand = c(0, 0),
                     breaks= c(1,2,3,4,5,6),
                     limits= c(0,6.1)) +
  geom_circle(aes(x0 = 2, y0 = 4.5, r = 1.5), color="#1b9e77", size = 0.5, inherit.aes = FALSE) +
  geom_circle(aes(x0 = 5, y0 = 2, r = 1.8), color="#ffa600", size = 0.5, inherit.aes = FALSE) +
  coord_fixed()+
  annotate("text", x = 2.1, y = 4.3, label = "1", size = 1.5, color="#3B3B3B") +
  annotate("text", x = 4.85, y = 1.85, label = "2", size = 1.5, color="#3B3B3B") +
  labs(
    title = "12-Nearest Neighbors Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  base_theme() +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        legend.key=element_blank(),
        legend.text = element_text(color = "#3B3B3B", size = 6),
        legend.background = element_rect(color = NA),
        axis.ticks = element_blank(),
        panel.grid.major = element_line(color="#656565", size = 0.05),
        axis.text = element_text(color="#3B3B3B", size = 6),
        axis.title.x = element_text(color="#3B3B3B", size = 6),
        axis.title.y = element_text(color="#3B3B3B", size = 6, angle = 90, vjust = 1),
        plot.title.position = "plot",
        plot.title =        element_text(size = rel(0.6)))
ggsave("./output/images/machine_learning/ml_knn_visualized.jpg", width = 4, height = 3)
```

This sounds easy enough. So what we does the algorithm adjust during training? On the one hand, it has to find optimal the number of neighbors it considers for prediction. In the case above, we can see within in the green group two points, which belong to Matt. By only considering the nearest two neighbors, we would alter our prediction for the new point to Matt. However, only considering the nearest two points might increase the chance that our results are driven by outliers. As we can see all other points apart from the two nearest belong to Liam. In the training process, the k-nearest algorithm needs to findthe optimal number of neighbors to rightly assign the highest number of texts to the right speaker.

The example above highlights one of the key trade offs in machine learning. When we define the number of neighbors too narrow, our predictions may not generalize well to unseen points and would be better of increasing the number of points. However, there might indeed be a lot of cases in which Matt says the same thing really fast in a short amount of time. Something like 'What would you like to do?', comes to mind. Thus choosing the right amount of neighbors to consider is a case for the bias variance trade off, which is present in all machine learning models. [EXPLANATION TRADEOFF]{.ul} During training the algorithm chooses the amount of neighbors, which maximizes the number of rightly classified speakers.

Another parameter, which k-nearest neighbors tunes during training process is the weighting function. The weighting function measures the distance between points, to calculate which ones are the nearest. In the example above we used the direct distance between points to classify them as neighbors (thus the circle). However, other functions of calculating the distance are possible. Luckily, we can also use the computer to choose the 'best' method to calculate the distance for us.

Taken together the model we end up with a mode that uses 13-neighbors to make the prediction of unseen text. To define the nearest neighbors the algorithm uses (weight_func inv). However, the total accuracy during training is not really high 0.119. As we have seen above, the number of texts from Matt is already VVV percent. Thus, our model did not help us predict the speaker and we would be better of predicting Matt for all spoken text. So lets turn to another model to see if it can help us to predict the actors of Critical Role.

# 6. Naive Bayes

While K-nearest Neighbor is fast to train. Fitting the data takes long time. Additionally, we don't really know what features of a text makes the model think that it was spoken by a certain actor. Moreover, the results were not satisfactory. Let us therefore try if another method does a better job of predicting the speaker.

One approach that is often used in spam classification tasks is called Naive Bayes. Its main idea is to calculate the probability that a given text is one of the classes (in our case one of the actors), given the words in the text. Additionally it includes the probability that a text is a given class in general. The class with the highest score is then chosen to be the predicated class. For example, the text may look as follows:

> Hello, everyone and good evening.

Naive Bayes then calculates the probability that the text is spoken by a certain actor, taking the probability that a given text is spoken by that actor and then multiplies the by the probability that the text is spoken by Matt given the words in the text. The actor with the highest values is then chosen as the predicted class. Intuitively you can this of this as giving all features the same weight. This makes sense when using text data. For this approach all features are equally important. Please note that the we don't use full sentences as we only look at the most predictive words to make the models comparable. Moreover, it is also not ideal to have numeric features like the speed talking in the analysis. However, in the interest of comparability we leave these features as we have done so far. If the naive Bayes model is our best one, or at least similar to the best, we can still rerun it with a better specification.

In terms of tuning parameters, naive Bayes has two. One the one hand, we need to adjust cases, which happen relatively rarely. The reason for this is the fact that when some features only happen rarely or never for some classes, will result in probabilities of zeros and ones. As it does not seem realistic that a certain text is certainly not spoken by a certain actor, we need to adjust the counts of these low-frequency features. We do so by adding small number to all of our features to ensure non zero probabilities. However, we don't know how big this small number should be. Similar to before, we can make the computer decide this problem for us. Moreover, we can make the computer decide on how smooth class boundaries should be. What does that mean? In contrast to our example above (with the linear k-nearest neighbors), our boundaries for the predictions can be smooth. However, how smooth you want these boundaries to be influences the predictions made, as can be seen in the bottom group. If we make the boundaries too smooth we can exclude the two points for Liam and include them in the Matt group. However, as we don't know whether this helps our overall predictions we let the computer tune it for us.

```{r naive_bayes, echo = FALSE}
graph %>% 
  ggplot(aes(x,y,color=actor)) + 
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77","#ffa600", "black")) +
  scale_y_continuous(expand = c(0, 0),
                     breaks= c(1,2,3,4,5,6),
                     limits= c(0,6.1)) +
  # Liam
  geom_curve(aes(x = 2.5, y = 3.2, xend = 1.8, yend = 4.3), curvature = 0.1, size = 0.2) +
  geom_curve(aes(x = 1.8, y = 4.3, xend = 1.7, yend = 4.7), curvature = -0.1, size = 0.2) +
  geom_curve(aes(x = 1.7, y = 4.7, xend = 2.8, yend = 4.7), curvature = -0.1, size = 0.2) +
  geom_curve(aes(x = 2.8, y = 4.7, xend = 3, yend = 5), curvature = 0.1, size = 0.2) +
  geom_curve(aes(x = 3, y = 5, xend = 0.7, yend = 5.4), curvature = 0.1, size =0.2) +
  geom_curve(aes(x = 0.7, y = 5.4, xend = 0.4, yend = 5), curvature = 0.1, size = 0.2) +
  geom_curve(aes(x = 0.4, y = 5, xend = 1.8, yend = 3.2), curvature = 0.1, size = 0.2) +
  geom_curve(aes(x = 1.8, y = 3.2, xend = 2.5, yend = 3.2), curvature = 0.1, size = 0.2) +
  # Matt
  geom_curve(aes(x = 1.9, y = 4.25, xend = 1.75, yend = 4.65), curvature = -0.1, size = 0.2, color = "#ffa600")  +
  geom_curve(aes(x = 1.75, y = 4.65, xend = 3, yend = 4.65), curvature = -0.1, size = 0.2, color = "#ffa600")  +
  geom_curve(aes(x = 3, y = 4.65, xend = 5, yend = 4), curvature = -0.1, size = 0.2, color = "#ffa600")  +
  geom_curve(aes(x = 5, y = 4, xend = 6.5, yend = 2), curvature = -0.1, size = 0.2, color = "#ffa600")  +
  geom_curve(aes(x = 6.5, y = 2, xend = 5.8, yend = 0.6), curvature = -0.1, size = 0.2, color = "#ffa600")  +
  geom_curve(aes(x = 5.8, y = 0.6, xend = 4.2, yend = 0.2), curvature = -0.1, size = 0.2, color = "#ffa600")  +
   geom_curve(aes(x = 4.2, y = 0.2, xend = 3, yend = 2.2), curvature = -0.1, size = 0.2, color = "#ffa600")  +
    geom_curve(aes(x = 3, y = 2.2, xend = 1.9, yend = 4.25), curvature = 0.1, size = 0.2, color = "#ffa600")  +
  coord_fixed()+
  labs(
    title = "Naive Bayes Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  base_theme() +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        legend.key=element_blank(),
        legend.text = element_text(color = "#3B3B3B", size = 6),
        legend.background = element_rect(color = NA),
        axis.ticks = element_blank(),
        panel.grid.major = element_line(color="#656565", size = 0.05),
        axis.text = element_text(color="#3B3B3B", size = 6),
        axis.title.x = element_text(color="#3B3B3B", size = 6),
        axis.title.y = element_text(color="#3B3B3B", size = 6, angle = 90, vjust = 1),
        plot.title.position = "plot",
        plot.title =        element_text(size = rel(0.6)))
ggsave("./output/images/machine_learning/ml_naive_bayes_visualized.jpg", width = 4, height = 3)

```

In our final model, the computer decided to have a smoothness of 1.211 and add 2.88 to the count of features. This results in an accuracy of 0.03. This in an improvement over our previous model. However, we still have not managed to obtain better predictions than just predicting Matt for every speaker. Moreover, the assumption that all features are equally important and independent from each other seems to be unrealistic. Moreover, so far, we have examined models, which have nice statistical properties but do not allow us for intuitive interpretation. Wouldn't it be nice to have simple if-then rules, which can be read without knowing all the mathematical sheneigans? This is what decision trees are for. They are used, for example, by banks to be able to explain to client when they didn't get a credit. So let's build one to see if they are better at predicting the speakers and to see which features are relevant for the different speakers.

# 7. Decision Tree

The most illustrative example of a machine learning model is a decision tree. The idea is simple: in the end when want to have number of if-then statement, which guide us to our prediction. Let's look at how this would look like for our example. In every simple terms we could make the simple rule to say: if the duration of a text is longer than three seconds, we classify it as spoken by Matt. If it is shorter than three seconds we classify it as Liam. However, as we have seen, there two points in the top left cluster of Liam points. As we have discussed earlier, these might be always the same short phrases said by Matt. Let's say the phrase would be 'roll for initiative' (indicating the beginning of combat). If Matt says this phrase often enough, the decision tree might come up with the more nuanced rule: if the text shorter than three seconds and includes the words "roll for" then predict Matt. This is way more illustrative than we two models we have used previously. However, does it also do a better job at predicting the speakers? Let's find out.

```{r decision_tree_one_line, echo = FALSE}
graph %>% 
  filter(actor != "New Point") %>% 
  ggplot(aes(x,y,color=actor)) + 
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77","#ffa600", "black")) +
  scale_y_continuous(expand = c(0, 0),
                     breaks= c(1,2,3,4,5,6),
                     limits= c(0,6.1)) +
  coord_fixed()+
  geom_vline(xintercept = 3,
             size = 0.15,
             color = "#3B3B3B",
             linetype = "dashed") +
  annotate("text", x = 2.5, y = 5.8, label = "Liam", size = 1.8, color="#3B3B3B") +
  annotate("text", x = 3.5, y = 5.8, label = "Matt", size = 1.8, color="#3B3B3B") +
  labs(
    title = "Decision Tree Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  base_theme() +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        legend.key=element_blank(),
        legend.text = element_text(color = "#3B3B3B", size = 6),
        legend.background = element_rect(color = NA),
        axis.ticks = element_blank(),
        panel.grid.major = element_line(color="#656565", size = 0.05),
        axis.text = element_text(color="#3B3B3B", size = 6),
        axis.title.x = element_text(color="#3B3B3B", size = 6),
        axis.title.y = element_text(color="#3B3B3B", size = 6, angle = 90, vjust = 1),
        plot.title.position = "plot",
        plot.title =        element_text(size = rel(0.6)))
ggsave("./output/images/machine_learning/ml_decision_tree_visualized_one.jpg", width = 4, height = 3)
```

So how does a decision tree come up with the final model. The idea to split our data in various steps, to narrow it down so far that we can make predictions. To decide on which variable to split first, the tree test, which split would help us best to differentiate between classes. So in the example the algorithm would check, whether splitting at a duration of 1 seconds, 2 seconds etc. Similarly, it would check whether words per seconds would be a good split. As we can see, splitting at words per seconds of three is actually better than splitting at number of words as we rightly classify two texts from Liam, which we attributed to Matt previously. Thus, the first split from the tree would be at a talking speed of three words per second.

In the next step, the algorithm would ask whether it can further split the sample to make better predictions. If we look at the graph our immediate response might be to say yes. We could split the data such that we make the right predictions for two points of Matt, while miss classifying one point of Liam. This would increase our overall accuracy (see below). However, what if the two points are the only two occurrence of these combinations of values for Matt? If this was the case, we could potentially miss classify many text, which belong to Liam. We would over-fit the data. To prevent this we can tell the algorithm that we don't want too complex trees (meaning to many splits) in our model. We do this by adding a complexity penalty to our algorithm. Doing so, results in our algorithm not preferring too complex models over simpler models, which do only a sightly worse predicting our speakers.

```{r decision_tree_multiple_line, echo = FALSE}
graph %>% 
  filter(actor != "New Point") %>% 
  ggplot(aes(x,y,color=actor)) + 
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77","#ffa600", "black")) +
  scale_y_continuous(expand = c(0, 0),
                     breaks= c(1,2,3,4,5,6),
                     limits= c(0,6.1)) +
  coord_fixed()+
  geom_hline(yintercept = 2.96,
             size = 0.15,
             color = "black",
             linetype = "dashed") +
    geom_hline(yintercept = 4.4,
             size = 0.15,
             color = "black",
             linetype = "dashed") +
    geom_hline(yintercept = 4.7,
             size = 0.15,
             color = "black",
             linetype = "dashed") +
  annotate("text", x = 7, y = 5.8, label = "Liam", size = 1.8, color="#3B3B3B") +
  annotate("text", x = 7, y = 4.55, label = "Matt", size = 1.8, color="#3B3B3B") +
  annotate("text", x = 7, y = 3.5, label = "Liam", size = 1.8, color="#3B3B3B") +
  annotate("text", x = 7, y = 2.5, label = "Matt", size = 1.8, color="#3B3B3B") +
  labs(
    title = "Decision Tree Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  base_theme() +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        legend.key=element_blank(),
        legend.text = element_text(color = "#3B3B3B", size = 6),
        legend.background = element_rect(color = NA),
        axis.ticks = element_blank(),
        panel.grid.major = element_line(color="#656565", size = 0.05),
        axis.text = element_text(color="#3B3B3B", size = 6),
        axis.title.x = element_text(color="#3B3B3B", size = 6),
        axis.title.y = element_text(color="#3B3B3B", size = 6, angle = 90, vjust = 1),
        plot.title.position = "plot",
        plot.title =        element_text(size = rel(0.6)))
ggsave("./output/images/machine_learning/ml_decision_tree_visualized_multiple.jpg", width = 4, height = 3)
```

So how do we decide, when to stop dividing the points? There are two major decision number on when to stop splitting the data further. On the one hand, we could say, that the number of points we need to have in each bucket should be at least a certain number. In our case we might say that making a prediction based on three points is too uncertain for our liking. One the other hand, we could say that we only like to split our data into so many different buckets. As we don't know before hand how our data looks like we will let your research assistant, the decision tree algorithm, come up what they think is best to generalize to unseen data.

In our final model, the algorithm decided on a tree that is 7 splits long. Moreover, it decided that there 11 is the minimum of points in each quadrant, to consider further splits. In order to reach the final tree a penalty for each split was induced of 4.43 wrongly classified speakers. This all sounds pretty theoretical so how does this look in practice? Unfortunately, a tree with seven decisions is too big to be shown here (but is available in github). However, we can look at one case to illustrate the idea. The first question the decision tree algorithm asks when it sees a new data point is whether the text has more than 133 characters. If that is the case, then we examines whether there are less than 248 lower characters in the text. If that is the case and the text contains the words, 'know', 'going', 'use' and 'oh' it asks whether the text contains the word 'just'. If it does not than it predicts Matt. If it does, the algorithm predicts Sam (right interpretation tfidf?). Hence, the two (hypothetical) sentences would result in the following predictions:

|                                                                     |            |
|---------------------------------------------------------------------|------------|
| Text                                                                | Prediction |
| You know what? It is going to use its ability on you.               | Matt       |
| You know what? I am just going to use bardic inspiration on myself. | Sam        |

This rule seems overly simplistic. However, other rules contain more text specific features like the number of periods in a text. But those do not illustrate the difference in prediction as nicely. So how does the algorithm do? In total it rightly assigns 20.7% of our texts to the right speaker. This is a big improvement over the Naive Bayes approach or also beats our prediction using k-Nearest Neighbors. However, it is still worse than predicting only Matt for all our classes. So can we do better?

# 8. Random Forest - Prediction + chosen Parameters

A further development of decision trees are random forests. The idea is to build many decision trees randomly (hence the name). In the first step, the algorithm randomly selects data from our training data (a process called bagging). The data is bootstrapped, meaning that each point can be chosen more than once. Next, the algorithm builds a decision tree but only chooses two random variable as candidates for the splits. For the next split, the algorithm again chooses two random variables and reaches a decision. It does so until the tree is finished. We then build many of these decisions trees to make our forest. To make our predictions, we let all our individual trees cast a vote on which prediction to make. The actor with the most votes is our prediction for this text. To evaluate the random forest we would use the texts not randomly selected into the new data set to obtain the accuracy of the random forest.

As the procedure is a bit abstract, let's talk about what at would mean for our case. In the first step, we build our new data set, which can contain multiple of the same dialogue. Next we start to build our first tree. So we randomly select two variables, between which we have to decide. Let's say we randomly selected the number of characters in a text and whether or not a text happens during combat. As we have seen in the previous section, we might come to the conclusion that the number of characters does better help us for deciding on the actor than knowing that a speech happens during combat. However, given that we have a new data set, this does not necessarily have to be the case. So after deciding, which split helps us better predicting the speaker, we split the data accordingly. Next, we again chose two random variables. Now, we have to decide whether the words per minute or the number or of commas in a text help us better for the prediction. This is likely not be a clear cut compared to the previous step as we also do not use the full training data. Let's assume we decide to split using the talking speed. We then would split the data accordingly. We continue with this process until the number in each leaf is smaller than a certain number and the decision tree is done. Afterwards we build many of these decision trees and let them vote to obtain our predictions.

Like in the decision tree, we can let the computer decide on the number of trees and the minimum number of data required to further splitting. Additionally, we let the computer decide between how many variables the random forest can choose between. This leaves us with three tuning parameters. As we have over 300 variables to chose from, the decision on which combinations of these values is our best bet for making our predictions, the training takes a really long time.

*self-derived think about again:* How could we think about the process graphically? One way to think about this algorithm is that we build many small boxes in which our data points are contained in. How does that potentially help with the results? In contrast to decision trees we cannot easily say why we decided on a certain actor and not somebody else. However, decision trees are really sensitive to the data. For example, what would happen if we sample only texts in which Matt speaks quickly? We would associate this feature with Matt, even though this would not necessarily be the case. If this were the case, a decision tree would base a lot of it predictions on this feature. This is especially true, if different variables are correlated ([why?]{.ul}). For example, the number of words in a text and duration over which it is spoken is likely to be correlated (correlation coefficient XXX). WHY BAD? The random forest is built using random data and random variables. Thus, they the individual trees are less likely to be driven by a high correlation between these two features. Hence, if the predictions by some trees may be driven by correlation, given that we calculate many more trees, we don't give their vote much say. Thus, we are more protected against the errors of the individual trees.

```{r random_forest, echo = FALSE}
graph %>% 
  filter(actor != "New Point") %>% 
  ggplot(aes(x,y,color=actor)) + 
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77","#ffa600", "black")) +
  scale_y_continuous(expand = c(0, 0),
                     breaks= c(1,2,3,4,5,6),
                     limits= c(0,6.1)) +
  coord_fixed()+
  labs(
    title = "Random Forest Visualized",
    subtitle = "Each Rectangle represents the prediction from a single tree",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  annotate("rect", xmin=0.5, xmax=3, ymin=3.3, ymax=5.3, alpha=0.1, fill="#1b9e77", color = "#1b9e77", size = 0.2) +
  annotate("rect", xmin=0.7, xmax=2.8, ymin=4.85, ymax=5.2, alpha=0.1, fill="#1b9e77", color = "#1b9e77", size = 0.2) +
  annotate("rect", xmin=4.5, xmax=5.5, ymin=2.7, ymax=3.5, alpha=0.1, fill="#1b9e77", color = "#1b9e77", size = 0.2) +
  annotate("rect", xmin=5.2, xmax=5.8, ymin=1.2, ymax=1.8, alpha=0.1, fill="#1b9e77", color = "#1b9e77", size = 0.2) +
  annotate("rect", xmin=4, xmax=6.5, ymin=0, ymax=3.1, alpha=0.1, fill="#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin=1.6, xmax=2.5, ymin=4.2, ymax=4.7, alpha=0.1, fill="#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin=5, xmax=6, ymin=0.5, ymax=2.5, alpha=0.1, fill="#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin=2, xmax=6.5, ymin=1, ymax=2, alpha=0.1, fill="#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin=2.5, xmax=3.5, ymin=4.7, ymax=5.5, alpha=0.1, fill="#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin=1.6, xmax=2.5, ymin=4.2, ymax=4.7, alpha=0.1, fill="#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin=1.5, xmax=6.5, ymin=0, ymax=5.5, alpha=0.1, fill="#ffa600", color = "#ffa600", size = 0.2) +
  base_theme() +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        legend.key=element_blank(),
        legend.text = element_text(color = "#3B3B3B", size = 6),
        legend.background = element_rect(color = NA),
        axis.ticks = element_blank(),
        panel.grid.major = element_line(color="#656565", size = 0.05),
        axis.text = element_text(color="#3B3B3B", size = 6),
        axis.title.x = element_text(color="#3B3B3B", size = 6),
        axis.title.y = element_text(color="#3B3B3B", size = 6, angle = 90, vjust = 1),
        plot.title.position = "plot",
        plot.title =        element_text(size = rel(0.6)))
ggsave("./output/images/machine_learning/ml_random_forest_visualized.jpg", width = 4, height = 3)
```

So what what does the computer decide upon and how did it do? In the final model, 1790 trees are used to make the predictions. At each split, 52 random variables are sampled to make a decision between. The splitting stops when there are 19 or fewer texts within a split. Predicting our test data set, the model only manage to get 21.5 percent of cases right. Again, we would do better by just predicting Matt for all texts.

# 9. XGBoost

Another approach for increasing the performance of decision trees are extremely boosted trees. They specifically designed for large data sets, so they might do a better job than our previous models. The idea is to improve the performance of the forest (an ensemble of trees) towards better prediction continuously instead of only randomly. In the first step, we make an initial prediction (say each class is similarly likely, meaning a probability of 20%). Afterwards we build a decision tree that minimizes the differences between our initial guess and the actual probability for each actor (0 or 1 as each text is spoken by one actor). Afterwards we build a new tree using our predicted probability for each point so far on as our initial guess. We continue doing so until, we computers says it can no longer improve the results. Intuitively you can think of this algorithm as adding trees to a forest, where each new tree tries to help our prediction by predicting parts, which were previously unaccounted for by the forest. Graphically, the result would look similar to the random forest.

How exactly this trees looks (how many predictors are randomly compared, how big the individual trees are and how many trees are built etc.), we leave again for the computer to decide. In our case, the computer decided that at each step 9(?) random variables are compared against each other and 70%(?) of the data is randomly sampled. Second, that the minimum number required for further splits is 29(?) texts. Additionally, our loss function needs to decrease by 0.00005(?) to allow for the split even if 29 or less texts are in a leaf. Last, our model contains XXX trees and the the weight given to the prediction of each of these trees is 0.9(?).

Lets see how our new model does. In total our new model only predicts with an accuracy of 0.185.

# 10. (Regularized) Regression - Line graphs

So far, the models failed to tell us the effect of certain characteristics on the probability of predicting a certain speaker. For example, we might be interested in how much does an increased talking speed influence our prediction that a text is spoken by a certain actor? Although, the decision tree gives us a good indication, which features of a text matter for prediction, we might still be interested in the exact influence. One way to estimate these effects, is to use a more simple model called OLS. The model estimates the effect of different variables on the probability of different speakers as follows:

EXPLAIN LINEAR MODEL MORE CLEARLY

$$Speaker_n = \beta_0 + \beta_1(words\ per\ second) $$

```{r regression, echo = FALSE}
graph %>% 
  filter(actor != "New Point") %>% 
  ggplot(aes(x,y,color=actor)) + 
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77","#ffa600", "black")) +
  scale_y_continuous(expand = c(0, 0),
                     breaks= c(1,2,3,4,5,6),
                     limits= c(0,6.1)) +
  coord_fixed()+
  labs(
    title = "Multinomoinal Regression Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  geom_abline(slope = 0.4, intercept = 0, size = 0.25, color = "#ffa600") +
  geom_abline(slope = 0.6, intercept = 2, size = 0.25, color = "#1b9e77") +
  base_theme() +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        legend.key=element_blank(),
        legend.text = element_text(color = "#3B3B3B", size = 6),
        legend.background = element_rect(color = NA),
        axis.ticks = element_blank(),
        panel.grid.major = element_line(color="#656565", size = 0.05),
        axis.text = element_text(color="#3B3B3B", size = 6),
        axis.title.x = element_text(color="#3B3B3B", size = 6),
        axis.title.y = element_text(color="#3B3B3B", size = 6, angle = 90, vjust = 1),
        plot.title.position = "plot",
        plot.title =        element_text(size = rel(0.6)))
ggsave("./output/images/machine_learning/ml_regression_visualized.jpg", width = 4, height = 3)
```

where speaker_n is the speaker we're predicting and the the beta are the effects of the different variable on the probability that the text belong to a speaker. If we plug the numbers in for each speaker, we chose the one with the highest probability. Using this we can estimate the effect of the characteristics of the text on probability that the text is spoken by certain actors. Under certain assumptions, we can be certain that the estimated effect is unbiased effect. For example, the model gives us that a text, which contains the word "you", has increases the probability that the text is spoken by Matt by XXX percent. Likewise, if the text contains the word "you", the likelihood for Liam increases by ZZZ. However, the model is not very good at predictions. Once our model is trained we only predict YYY% of speakers correctly. The reason for this is the fact that the regression over fits by construction as it is more aimed at parameter estimation compared to prediction. What this means is that when we look at different subsets of the data, we may get really different values out of our estimation. Thus, our success at predicting speakers will vary quite a lot depending on the subset our estimator was constructed on. What we can do instead is a add some bias to the estimation to increase our predictive power. However, once we do so we can no longer interpret the numbers as X increases the chance of Z by V%. Still, it will increase our predictive power.

The idea is to add some regularizing terms to our regression, as follows:

$$Speaker_n = \beta_0 + \beta_1(words\ per\ second) + \lambda(\sum\alpha\beta) $$

where the best form of how the extra parameters enters can found out by the computer. Our final model chooses a lambda of 1.45. We manage to rightfully predict 0.236 of all speakers, which is best we have managed as of yet. This is still worse than only predicting Matt for all cases. So let's try one last model to see if we can make it work out of the box.

```{r regularized_regression, echo = FALSE}
graph %>% 
  filter(actor != "New Point") %>% 
  ggplot(aes(x,y,color=actor)) + 
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77","#ffa600", "black")) +
  scale_y_continuous(expand = c(0, 0),
                     breaks= c(1,2,3,4,5,6),
                     limits= c(0,6.1)) +
  coord_fixed()+
  labs(
    title = "Regularized Regression Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  geom_abline(slope = 0.4, intercept = 0, size = 0.15, color = "#ffa600") +
  geom_abline(slope = 0.3, intercept = 0.3, size = 0.35, color = "#ffa600") +
geom_segment(aes(x = 6.5, y = 2.55, xend = 6.5, yend = 2.35),
                  arrow = arrow(length = unit(0.05, "cm")),
             color = "#ffa600",
             size = 0.25) +
  geom_abline(slope = 0.6, intercept = 2, size = 0.15, color = "#1b9e77") +
  geom_abline(slope = 0.8, intercept = 1.5, size = 0.25, color = "#1b9e77") +
  geom_segment(aes(x = 5.5, y = 5.5, xend = 5.2, yend = 5.5),
                  arrow = arrow(length = unit(0.05, "cm")),
             color = "#1b9e77",
             size = 0.25) +
  base_theme() +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        legend.key=element_blank(),
        legend.text = element_text(color = "#3B3B3B", size = 6),
        legend.background = element_rect(color = NA),
        axis.ticks = element_blank(),
        panel.grid.major = element_line(color="#656565", size = 0.05),
        axis.text = element_text(color="#3B3B3B", size = 6),
        axis.title.x = element_text(color="#3B3B3B", size = 6),
        axis.title.y = element_text(color="#3B3B3B", size = 6, angle = 90, vjust = 1),
        plot.title.position = "plot",
        plot.title =        element_text(size = rel(0.6)))
ggsave("./output/images/machine_learning/ml_regularized_regression_visualized.jpg", width = 4, height = 3)
```

# 11. Black Box: SVM

One last model, although many more a available, we can use to predict the actor from the text are support vector machines (SV). The idea is rather simple. The goal is to find a line which separates one actor from all the others. If you then find one such line for each actor, we can use those to make prediction for unseen text. As we have seen when applying OLS to the data, the result is rater unsatisfactory. So how do SVs try to make better better predictions than our previous attempts?

Let's suppose we wanted to fit a line between the data we have examined so far. The result is not really satisfactory (see previous section). So let's examine how SVs might make better prediction. To make things easier to follow, we only look at the speed of talking in the following. If we were to graph these points on a line, we would see that cannot easily separate Liam from Matt. However, what SVs do is that they add higher dimensions to the data to try to fit a line to distinguish between the two actors. In this case that would mean to, for example, add another axis with the speed of talking squared. Afterwards the algorithm tries to fit a line between the two speakers to see if it can better fit the data. As we can see below, adding a squared to term does help us to achieve a similar result to what we have seen above by adding the words per second variable. So we can think of the algorithm to add different versions of the existing variables to try to find a line that separates one speaker from all the other speakers.

```{r svm, echo = FALSE}
graph %>% 
  filter(actor != "New Point") %>%
  mutate(squared = x*x) %>% 
  ggplot(aes(x,squared,color=actor)) + 
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77","#ffa600", "black")) +
  scale_y_continuous(expand = c(0, 0),
                     limit = c(0,50)) +
  scale_x_continuous(limit = c(0,8)) +
  geom_vline(xintercept= 3.6, size = 0.15) +
  annotate("text", x = 2, y = 47, label = "Liam", size = 1.8, color="#3B3B3B") +
  annotate("text", x = 6, y = 47, label = "Matt", size = 1.8, color="#3B3B3B") +
  coord_fixed()+
  labs(
    title = "Support Vector Machine Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Time in Second "
  ) +
  base_theme() +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        legend.key=element_blank(),
        legend.text = element_text(color = "#3B3B3B", size = 6),
        legend.background = element_rect(color = NA),
        axis.ticks = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major = element_line(color="#656565", size = 0.05),
        axis.text = element_text(color="#3B3B3B", size = 6),
        axis.title.x = element_text(color="#3B3B3B", size = 6),
        axis.title.y = element_text(color="#3B3B3B", size = 6, angle = 90, vjust = 1),
        plot.title.position = "plot",
        plot.title =        element_text(size = rel(0.6)))
ggsave("./output/images/machine_learning/ml_support_vector_visualized.jpg", width = 4, height = 3)
```

As we can see in the graph above, adding a squared term does not help us to perfectly predict the speaker. Moreover, it seems unlikely to find a dimension which, distinguishes perfectly one speaker from all the others. Additionally, this line may not generalize well to text it has not seen before. Thus, we want only want to find the line, which separates the two speakers best. Therefore we allow the computer to miss classify some texts to the wrong speaker. We can also allow the computer to determine how much lean way it wants to give itself. Together the two factors, to determine the polynomial (the degree and the scaling factor of the polynomial), the computer has to determine the best fit for the three values.

In the end, the computer chooses a cost of XXX for each miss classified sample. For the polynomial it decided on a degree of ZZZ and a scaling factor of YYY. This resulted in a accuracy of XYZ.

# 12. Investigate two Best Models: Variable Importance and Confusion Matrix

-   Random forest and SVM: take really long time

-   decision tree: would be really big and does add much -\> take an improvement from it

    -   -\> designed for large data sets: Maybe not large enough? (Use argument later for tuning with number of words)

-   compare regularized regression and XGBoost

-   Table -\> combine fit_model.csv into df -\> add Matt -\> into line graph with Model on y axis and accruacy on x-axis

| Model                | Accuracy |
|----------------------|----------|
| just guessing "Matt" | 0.284    |
| knn                  | 0.119    |
| bayes                | 0.03     |
| decision             | 0.207    |
| RF                   | 0.215    |
| XG                   | 0.185    |
| OLS                  | \-       |
| regularized          | 0.236    |
| SVM                  | \-       |

```{r regularized_actor, echo = FALSE, warning=FALSE, message = FALSE}
regularized_predictions %>%
   mutate(actor_guest = as.factor(actor_guest),
        .pred_class = as.factor(.pred_class)) %>% 
conf_mat(actor_guest, .pred_class) %>%
 pluck(1) %>%
 as_tibble() %>% 
mutate(prediction = ifelse(Prediction == Truth,"rightly classified","wrongly classified")) %>% 
group_by(Truth,prediction) %>% 
summarise(sum(n)) %>% 
rename(count = `sum(n)`,
 actor = Truth) %>%
 group_by(actor) %>% 
mutate(percent = count/(sum(count)*100)) %>% 
filter(prediction =="rightly classified") %>% 
  ggplot(aes(x = reorder(actor,percent), y = percent, fill = prediction)) +
  geom_bar(stat = "identity", fill = "#1b9e77") +
  coord_flip() +
  scale_y_continuous(
    expand = c(0, 0),
    position = "right"
  ) +
  labs(
    title = "Regularized Regression: Right and Wrong Predictions by Actor",
    subtitle = "Percent of total prediction by actor",
  ) +
  bar_chart_theme()

ggsave("./output/images/machine_learning/ml_regularized_actor.jpg", width = 4, height = 3)
```

-   We see that regularized model does a really good job predicting Matt (0.729649855 of total right predicting), even though regularized[Matt] nearly equal to sum(xgboost)

```{r xgboost_actor, echo = FALSE, warning=FALSE, message = FALSE}
naive_bayes_predictions %>%
   mutate(actor_guest = as.factor(actor_guest),
        .pred_class = as.factor(.pred_class)) %>% 
conf_mat(actor_guest, .pred_class) %>%
 pluck(1) %>%
 as_tibble() %>% 
mutate(prediction = ifelse(Prediction == Truth,"rightly classified","wrongly classified")) %>% 
group_by(Truth,prediction) %>% 
summarise(sum(n)) %>% 
rename(count = `sum(n)`,
 actor = Truth) %>%
 group_by(actor) %>% 
mutate(percent = count/(sum(count)*100)) %>% 
filter(prediction =="rightly classified") %>% 
  ggplot(aes(x = reorder(actor,percent), y = percent, fill = prediction)) +
  geom_bar(stat = "identity", fill = "#1b9e77") +
  coord_flip() +
  scale_y_continuous(
    expand = c(0, 0),
    position = "right"
  ) +
  labs(
    title = "DATA FRAME TO BE EDITED!!!!!!!! XGBoost: Right and Wrong Predictions by Actor",
    subtitle = "Percent of total prediction by actor",
  ) +
  bar_chart_theme()

ggsave("./output/images/machine_learning/ml_xgboost_actor.jpg", width = 4, height = 3)
```

-   More diversified = better (0.593483257 of all right predictions = Matt)

# 13. Conclusion
