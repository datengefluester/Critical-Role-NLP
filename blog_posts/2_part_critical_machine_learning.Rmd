---
title: "Critical Machine Learning: Prediciting Critical Role Actors from Text. Part 2"
always_allow_html: true
output:
  html_document: default
  md_document: default
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_format = "all") })
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
# tell Rmarkdown to run in root folder of project
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

# where figures for rendering the document are saved
knitr::opts_chunk$set(
  fig.path = "markdown_figs/"
)


# packages
library(dplyr) # for data preparation
library(tidyr) # for data preparation
library(stringr) # for data preparation
library(tidytext) # for rearrange facet + bar charts
library(ggplot2) # for graphs
library(kableExtra) # for table
library(purrr) # for pluck command
library(yardstick) # for confusion matrix
library(ggforce) # for drawing circles in the knn-graph
library(data.table) # for faster reading in data
```

```{r themes, include=FALSE}
# graph themes
source("./scripts/themes.R")
```

```{r data, echo = FALSE}
# ------  read in every data frame from the 'machine_learning' folder
path <- "./data/machine_learning/"
# folders
folders_path <- list.files(path = path, pattern = "", full.names = TRUE)
folders_path <- str_replace(folders_path, pattern = "//", replacement = "/")

# extract files and import them
for (i in 1:length(folders_path)) {
  # read in every csv-file in a folder into tibble
  tmp <- folders_path[i]
  files <- list.files(path = paste(tmp), pattern = ".csv")
  files_path <- paste(folders_path[i], "/", files, sep = "")
  file_name <- str_replace(files, ".csv", "")
  myfiles <- lapply(files_path, fread)
  # split tibble into seperate data frames
  for (df in 1:length(myfiles)) {
    tmp <- myfiles[[df]] %>% as.data.frame()
    assign(paste(file_name[df]), tmp)
    rm(tmp)
  }
}
# clean up
rm(df, files, file_name, files_path, folders_path, i, myfiles, path)


# ------  accuracy for all models combined
accuracy_overview <- decision_acc %>%
  bind_rows(., knn_acc) %>%
  bind_rows(., naive_bayes_acc) %>%
  bind_rows(., regularized_acc) %>%
  bind_rows(., rf_acc) %>%
  bind_rows(., xgboost_acc) %>%
  bind_rows(., xgb_tune_acc) %>%
  bind_rows(., multinominal_acc)

rm(decision_acc, knn_acc, naive_bayes_acc, regularized_acc, rf_acc, xgboost_acc, xgb_tune_acc, multinominal_acc)

# ------ time and turns by actor
actor_words_time_percent <- read.csv("./data/descriptive_analysis/actor_words_time.csv")
actor_words_time_percent <- actor_words_time_percent %>%
  select(1, 4, 10) %>%
  pivot_longer(
    cols = c("words_percent", "turns_percent"),
    names_to = "variable",
    values_to = "percent"
  ) %>%
  mutate(
    variable = str_replace(variable, "words_percent", "Words"),
    variable = str_replace(variable, "turns_percent", "Turns")
  )

# add 'just guessing Matt' to accuracy overview
percent_turns_matt <- actor_words_time_percent %>%
  filter(actor_guest == "Matt" & variable == "Turns") %>%
  mutate(percent = percent / 100) %>%
  mutate(
    rightly_classified = round(percent * nrow(rf_predictions)),
    wrongly_classified = (1 - percent) * nrow(rf_predictions)
  )

accuracy_overview <- accuracy_overview %>%
  add_row(
    model = "Only Guessing Matt",
    roc_auc = NA,
    accuracy = percent_turns_matt$percent[1],
    rightly_classified = percent_turns_matt$rightly_classified[1],
    wrongly_classified = percent_turns_matt$wrongly_classified[1]
  )

rm(percent_turns_matt)

# ------ example data
example <- read.csv("./data/descriptive_analysis/example.csv")
example_ml <- example_ml %>%
  select(actor_guest, text, time_in_sec, words_per_minute, arc, segment, combat) %>%
  mutate(segment = str_to_title(segment)) %>%
  rename(
    Actor = actor_guest,
    Text = text,
    `Time in Sec` = time_in_sec,
    `Words per Minute` = words_per_minute,
    Arc = arc,
    Segment = segment,
    Combat = combat
  )

# ------ Cross Validation Graph
nrows <- 10
cross_validation <- expand.grid(y = 1:nrows, x = 1:nrows)

cross_validation <- cross_validation %>%
  mutate(train_test = "Train Data") %>%
  mutate(train_test = replace(train_test, x == y, "Test Data")) %>%
  mutate(train_test = as.factor(train_test))

# ------ Example graph to visualize the algorithms
set.seed(1000)
Liam <- rep(c("Liam"), each = 10)
Liam_x <- rnorm(10, 2, 0.8)
Liam_y <- rnorm(10, 5, 0.7)
Liam_df <- data.frame(
  actor = Liam,
  x = Liam_x,
  y = Liam_y
)
Matt <- rep(c("Matt"), each = 10)
Matt_x <- rnorm(10, 5, 0.5)
Matt_y <- rnorm(10, 2, 0.9)
Matt_df <- data.frame(
  actor = Matt,
  x = Matt_x,
  y = Matt_y
)
graph <- rbind(Liam_df, Matt_df)
# add noise
graph <- graph %>%
  add_row(actor = "Matt", x = 1.8, y = 4.6) %>%
  add_row(actor = "Matt", x = 2.2, y = 4.5) %>%
  add_row(actor = "Liam", x = 5, y = 3) %>%
  add_row(actor = "Liam", x = 5.2, y = 3.2)

# clean up
rm(Liam, Liam_x, Liam_y, Matt, Matt_x, Matt_y, nrows, Matt_df, Liam_df)
```

## Introduction

In the last post, we discussed how we prepared the Critical Role subtitle data to predict the actors. First, we split the data into training and testing data sets. Second, given the imbalance of the data, we down sampled the data. Third, due to computational considerations, we decided to only include the 300 most predictive words. In this post, we try to predict the actor from the
text. We discuss different machine learning algorithms available and see well they can predict the speakers from the text.

## K-Nearest Neighbors

As a first attempt we can run the 'k-nearest neighbors'-algorithm to predict the speakers. As the name suggests, the algorithm classifies texts by examining the k-nearest texts. To make a prediction, it uses the speaker, who is present most within these texts. This may a bit counter-intuitive when it comes to text. However, we can illustrate the approach by looking at number of words of a text and duration over which it was spoken. The graph below plots some imaginary data for two of the actors (Matt and Liam). For new texts, the algorithm chooses the speaker, which is occurs most within the 12 nearest points. For point one this means that more points belong to Liam compared to Matt. Thus, the algorithm predicts that the point belongs to
'Liam' as well. For point two, the opposite applies and the algorithm predicts 'Matt' as the speaker of the text.

```{r knn_graph, echo = FALSE}
graph %>%
  add_row(actor = "New Point", x = 2, y = 4.5) %>%
  add_row(actor = "New Point", x = 5, y = 2) %>%
  ggplot(aes(x, y, color = actor)) +
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77", "#ffa600", "black")) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = c(1, 2, 3, 4, 5, 6),
    limits = c(0, 6.1)
  ) +
  geom_circle(aes(x0 = 2, y0 = 4.5, r = 1.5), color = "#1b9e77", size = 0.5, inherit.aes = FALSE) +
  geom_circle(aes(x0 = 5, y0 = 2, r = 1.8), color = "#ffa600", size = 0.5, inherit.aes = FALSE) +
  coord_fixed() +
  annotate("text", x = 2.1, y = 4.3, label = "1", size = 1.5, color = "#3B3B3B") +
  annotate("text", x = 4.85, y = 1.85, label = "2", size = 1.5, color = "#3B3B3B") +
  labs(
    title = "12-Nearest Neighbors Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  base_theme() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.key = element_blank(),
    legend.text = element_text(color = "#3B3B3B", size = 6),
    legend.background = element_rect(color = NA),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(color = "#656565", size = 0.05),
    axis.text = element_text(color = "#3B3B3B", size = 6),
    axis.title.x = element_text(color = "#3B3B3B", size = 6),
    axis.title.y = element_text(color = "#3B3B3B", size = 6, angle = 90, vjust = 1),
    plot.title.position = "plot",
    plot.title = element_text(size = rel(0.6))
  )
ggsave("./output/images/machine_learning/ml_knn_visualized.jpg", width = 4, height = 3)
```

In the last post we discussed that we can think of machine learning algorithms as machines consisting of many gearing wheels. Different configuration of the wheels results in different prediction. The goal of training an algorithm is to find the configuration which results in the most accurate predictions. With
that in mind, which wheels does the k-nearest neighbors algorithm adjust during training?

On the one hand, the algorithm must find optimal the number of neighbors it considers for its prediction. For the upper left cluster of points, we can see two points which belong to Matt. By only considering the nearest two neighbors, we would alter our prediction for point one to 'Matt'. Yet, only considering the nearest two points might mean that our results are driven by chance. As we can see, all other points belong to Liam. If an algorithm focuses too little on the patterns in the training data, it oversimplifies the model. Doing so, it does not pick up relevant features for our speakers (bias). For the case above, this could mean that we look at the nearest 24 points. This would mean that the algorithm fails to distinguish between the two groups of points. In the training
process, the k-nearest algorithm finds the optimal number of neighbors. Fortunately, we can let the algorithms decide, which number achieves the best predictions during training.

The example above highlights one of the key trade-offs in machine learning. When the k-nearest-neighbor algorithm only looks at a few of the nearest points, its predictions may not generalize well. If that is the case it would be better to increase the number of points. Still, there may be many cases in which Matt says the same thing in a short amount of time. One example might be, asking for the damage done by a player followed by 'How do you want to do so this?'. Thus, choosing the right number of neighbors to consider is a case for the bias variance trade off, which is present in all machine learning models.

On the other hand, the algorithm also searches for its best configuration is the weighting function. The weighting function determines how the algorithm measures the distance between points. In the example above, we used the direct distance between the points to classify them as neighbors. But other measures are available. Luckily, we can also use the computer to determine which method works best for our texts.

After training, our k-nearest neighbor model uses 13 neighbors to make its prediction. To define the nearest neighbors, it uses an inverse distance function. The function gives less weight to points further away from new points. Yet, the total accuracy of the model is not high (0.119). This means
we only predict the right speaker for 1.9 percent of texts. As we have seen in the last blog post, the number of texts belonging to Matt is already 28.4 percent. Thus, our model did not help us predict the speaker, and we would be better off predicting Matt for all spoken text.

In addition, there are two disadvantages to using the k-nearest-neighbors algorithm. First, while the algorithm is fast to train, it takes long to make its predictions. Second, the algorithm does not tell us which features influenced its decision. Taken together, the results were not satisfactory and do not allow
an easy interpretation. Luckily, we turn to other algorithms to see if they can help us to predict the actors of Critical Role.

## Naive Bayes

One algorithm often used in text classification is called Naive Bayes. Its main idea is to calculate the probability that a given text belongs to a class (in our case one of the actors), given the words in the text. Moreover, it includes the probability that a text is a given class in general (think of the 28.4 percent of
texts from Matt). The actor with the highest probability is then chosen to be the prediction for the text. Let's put some flesh on the theoretical bones and look at an example:

> Hello, everyone and good evening to tonight's episode of Critical Role.

In the first step, the algorithm calculates the probability that the text belongs to a certain actor in general. Recall, that we have ten actors and have down sampled the data, so each actor covers twenty percent of the training data. Thus, the algorithm starts with a probability for each actor of 20 percent. In the second step, it adds or subtracts the probability for each actor given the words in the text. Depending on how likely the actors are to use certain words the probability in increased or decreased. As 'tonight's episode of Critical Role' indicates the beginning of the show, it is likely to be said by Matt. Hence, the algorithm should assign the highest probability to him and choose him for its prediction.

The Naive Bayes algorithm has two tuning parameters, which is needs to configure during training. On the one hand, it needs to adjust for features, which happen relatively rarely. We must do this since some words are rarely spoken for some speakers. If certain words are only said by one or two actors, this will result in probabilities of zero or 1. As it is realistic that a text is not spoken by a certain actor with 100 or 0 percent certainty, the counts of these low-frequency words need to be adjusted. To do so, the algorithm adds a small amount to the count of all features to ensure realistic probabilities. As we don't know how big this small number should be, we can let the algorithm decide the number by itself.

On the other hand, the computer needs to decide how smooth class boundaries should be. What does that mean? In contrast to the k-nearest neighbors algorithm, our boundaries for the predictions can be smooth (see picture below). The smoothness of the boundaries influences the predictions. If we make the boundaries too smooth, we could exclude the two points for Liam in the bottom right cluster and include them in the Matt one. However, as we don't know whether this helps our predictions, we let the computer tune it for us.

```{r naive_bayes, echo = FALSE}
graph %>%
  ggplot(aes(x, y, color = actor)) +
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77", "#ffa600", "black")) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = c(1, 2, 3, 4, 5, 6),
    limits = c(0, 6.1)
  ) +
  geom_curve(aes(xend = 3, yend = 0, x = 7, y = 5), curvature = 0.5, size = 0.1,
             linetype = 2, colour = "#3B3B3B") +
    annotate("text", x = 2.5, y = 5.8, label = "Liam", size = 1.8, color = "#3B3B3B") +
  annotate("text", x = 6.5, y = 3.5, label = "Matt", size = 1.8, color = "#3B3B3B") +
  coord_fixed() +
  labs(
    title = "Naive Bayes Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  base_theme() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.key = element_blank(),
    legend.text = element_text(color = "#3B3B3B", size = 6),
    legend.background = element_rect(color = NA),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(color = "#656565", size = 0.05),
    axis.text = element_text(color = "#3B3B3B", size = 6),
    axis.title.x = element_text(color = "#3B3B3B", size = 6),
    axis.title.y = element_text(color = "#3B3B3B", size = 6, angle = 90, vjust = 1),
    plot.title.position = "plot",
    plot.title = element_text(size = rel(0.6))
  )
ggsave("./output/images/machine_learning/ml_naive_bayes_visualized.jpg", width = 4, height = 3)
```

To get its prediction, the Naive Bayes algorithm needs to assume that all features are independent. You can think of this as assuming that all words are
equally important. While this assumption may often unreasonable, it widely used in text classification. Yet, it is not ideal to have numeric features like the speed talking in our analysis. But, in the interest of comparability, we include these features as we have done so far. If Naive Bayes yields the best predictions, or at least close to that, we can still rerun it with a better specification.

In our trained Naive Bayes model, the computer decided to have a smoothness of 1.211 and adds 2.88 to the count of features. The later means that 2.88 are added to the count of our 318 features. The former tells us how smooth the boundaries are (the line in the picture above), however, they are not easily interpretable. Yet, our Naive Bayes model only achieves an accuracy of 0.03. This means that the algorithm only predicts the right speaker in three percent of cases. This is a decrease compared to the nearest neighbor model and simply guessing Matt for every text is still our best
approach.

Moreover, neither of the two models we examined so far allow us for intuitive interpretation. Wouldn't it be nice to have some simple if-then rules, which can be read without knowing all the mathematical shenanigans? This is what decision trees allow us to do. Among other applications, they are used by banks to be able to explain to clients why they didn't get a credit. So, let's build a decision tree to see which features are relevant for the predictions.

## Decision Tree

The most illustrative example of a machine learning model is a decision tree. After the model is trained, we get a number of if-then statement, which guide the predictions. Let's look at how this would look like for our example for differentiating between Matt and Liam. In very simple terms, we could make the simple rule to say: if the duration of a text is longer than three seconds,
we classify it as spoken by 'Matt'. If it is shorter than three seconds, we classify it as 'Liam'. Applying this simple rule, we only miss classify the two familiar points in each cluster (see picture below).

```{r decision_tree_one_line, echo = FALSE}
graph %>%
  filter(actor != "New Point") %>%
  ggplot(aes(x, y, color = actor)) +
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77", "#ffa600", "black")) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = c(1, 2, 3, 4, 5, 6),
    limits = c(0, 6.1)
  ) +
  coord_fixed() +
  geom_vline(
    xintercept = 3,
    size = 0.15,
    color = "#3B3B3B",
    linetype = "dashed"
  ) +
  annotate("text", x = 2.5, y = 5.8, label = "Liam", size = 1.8, color = "#3B3B3B") +
  annotate("text", x = 3.5, y = 5.8, label = "Matt", size = 1.8, color = "#3B3B3B") +
  labs(
    title = "Decision Tree Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  base_theme() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.key = element_blank(),
    legend.text = element_text(color = "#3B3B3B", size = 6),
    legend.background = element_rect(color = NA),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(color = "#656565", size = 0.05),
    axis.text = element_text(color = "#3B3B3B", size = 6),
    axis.title.x = element_text(color = "#3B3B3B", size = 6),
    axis.title.y = element_text(color = "#3B3B3B", size = 6, angle = 90, vjust = 1),
    plot.title.position = "plot",
    plot.title = element_text(size = rel(0.6))
  )
ggsave("./output/images/machine_learning/ml_decision_tree_visualized_one.jpg", width = 4, height = 3)
```

Yet, the decision tree can come up with the more nuanced rules. In first step, it could decide on our proposed rule. In the second step it could say, 'and
if a text is longer than three seconds and is spoken at three words per seconds or faster, predict 'Liam'. Using the new rule, we only rightly assign the two points in the bottom right cluster to Liam. The picture below visualizes the rule.

```{r decision_tree_two_lines, echo = FALSE}
graph %>%
  filter(actor != "New Point") %>%
  ggplot(aes(x, y, color = actor)) +
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77", "#ffa600", "black")) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = c(1, 2, 3, 4, 5, 6),
    limits = c(0, 6.1)
  ) +
  coord_fixed() +
  geom_vline(
    xintercept = 3,
    size = 0.15,
    color = "#3B3B3B",
    linetype = "dashed"
  ) +
  geom_curve(aes(x = 3, y = 2.96, xend = 7, yend = 2.96), curvature = 0, size = 0.12, color = "#3B3B3B", linetype = "dashed") +
  annotate("text", x = 2.5, y = 5.8, label = "Liam", size = 1.8, color = "#3B3B3B") +
  annotate("text", x = 3.5, y = 5.8, label = "Liam", size = 1.8, color = "#3B3B3B") +
  annotate("text", x = 3.4, y = 2.8, label = "Matt", size = 1.8, color = "#3B3B3B") +
  annotate("text", x = 3.4, y = 4.5, label = "1.", size = 2, color = "#3B3B3B") +
  annotate("text", x =7, y = 3.5, label = "2.", size = 2, color = "#3B3B3B") +
  labs(
    title = "Decision Tree Visualized (Continued)",
    subtitle = "",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  base_theme() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.key = element_blank(),
    legend.text = element_text(color = "#3B3B3B", size = 6),
    legend.background = element_rect(color = NA),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(color = "#656565", size = 0.05),
    axis.text = element_text(color = "#3B3B3B", size = 6),
    axis.title.x = element_text(color = "#3B3B3B", size = 6),
    axis.title.y = element_text(color = "#3B3B3B", size = 6, angle = 90, vjust = 1),
    plot.title.position = "plot",
    plot.title = element_text(size = rel(0.6))
  )
ggsave("./output/images/machine_learning/ml_decision_tree_visualized_two.jpg", width = 4, height = 3)
```

How could we turn our naive approach into a more systematic one? To decide how split to the data first, the decision tree algorithm tests all potential splits. Then it examines which one achieves the best differentiation between classes. For our example, the algorithm would check, whether splitting at 1 seconds or 2 seconds (or more) would be best to distinguish between Matt and Liam.
Likewise, it would check for potential splits at a certain number of words per seconds.

Looking at the picture above we can see that, splitting the data at three words per seconds differentiates the most between the two actors. We rightly classify two texts by Liam, which we attributed to Matt previously. Thus, the first split from the tree would be at a talking speed of three words per second.

In the next step, the algorithm would try to split each side again to improve its predictions. If we look at the bottom part of the graph, we see that it only contains point, which belong to Matt. Therefore, we do not split it any further. However, there are two points in the top half of the graph which the algorithm
attributes to Liam, which belong to Matt. However, there no single split, which could help us capture both points as all splits result in most points belonging to Liam. Therefore, the algorithm would not split the data any further.

It seems unlikely that different speakers can always be separated as nicely as in our example. Moreover, even if we could separate the speakers better it may not be a good idea to do so (since it may cause over-fitting). So how does a decision tree decide on when to stop divide the data further?

There are two approaches to decide when to stop splitting the data further. On the one hand, we could say that the number of points we need to have in each segment should be at least a certain number. In our case we might say that making a prediction based on ten points is too uncertain for our liking. If this were the case, we would prefer splitting the data at a duration of four seconds per text as it leaves twelve points in each segment. One the other hand, we could simply say that we only like to split our data so many times. Additionally, we can tell the algorithm that we prefer fewer splits over too many. We do this by adding a complexity penalty to the algorithm. Like before, since we don't know which configuration works best, we let the decision tree algorithm tune itself.

In our final model, the decision tree is 7 splits long. To consider further splits, it requires a minimum of 11 points in an area. For each split it induces a penalty of 4.43 wrongly classified speakers.

Unfortunately, a tree with seven decisions is too big to be shown here (but is available on github). However, we can look at one path to illustrate the idea. The first question our final decision tree model asks is whether the text has more than 133 characters (1). If that is the case, then it examines whether there are less than 248 lower characters in the text (2). If this applies as well, the model examines whether the text contains the words, 'know' (3), 'going' (4), 'use' (5) and 'oh' (6). If all of these are included in the text, it asks whether the text additionally contains the word 'just' (7). If it does not, it
predicts that the text is spoken by 'Matt'. If it does, the algorithm predicts 'Sam'.

+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+
| Text                                                                                                                                                                                                                                       | Prediction |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+
| and with that, you know what? it's going to go ahead and do this. the entity then moves backwards. pike and scanlan oh, you already used your reaction to do cutting words, so you don't get that. pike, go ahead and make a melee attack. | Matt       |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+
| oh, well, we don't need to keep saying these things. you know, if you guys would like to stay just for a few days, because we're going to be leaving.                                                                                      | Sam        |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+

This rule seems simplistic. However, other rules contain more text specific features, like the number of periods in a text. But those do not illustrate the differences in prediction as nicely. Taken everything together, how accurately does the algorithm predict the speakers? In total, it assigns 20.7% of the texts to the right speaker. This is a big improvement over our two previous models. However, it is still worse than predicting only Matt for all our classes. Fortunately for us, there are other methods available, which extend the decision tree algorithm. Let's see if we can use them to improve our predictions.

## Random Forest

One extension of decisions trees are called random forests. As the name suggests, the algorithm builds many decision trees randomly. In the first step, it randomly selects parts of our training data (a process called bagging). The data is bootstrapped, meaning that each point can be chosen more than once. In the second step, the algorithm builds a decision tree from the new data set but only chooses several random variables as candidates for each split. After each split, the algorithm chooses another set of random variables and chooses the best one for the split. It does so until the tree is finished. It then builds many of these decisions trees to create a forest. To make a prediction, the algorithm lets all our individual trees cast a vote on which prediction to make. The actor with the most votes becomes the prediction for the new point.

As the procedure is a bit abstract, let's talk about what the process would mean for our Liam-versus-Matt example. In the first step, we build our new data set, which might contain copies of the same dialogue. In the second step, we start to build our first tree. To do so, we randomly select two variables, between which we chose a split that best distinguishes between the
two actors. Let's say we selected the number of characters in a text and whether a text happens during combat. We might conclude that splitting the data at 120 characters does better job than knowing whether a text happens during combat. Hence, we split the data if it contains more than 120 characters.

After splitting the data, we again chose two random variables. Now, we must decide whether the words per minute or the number of commas in a text improves our predictions more and split the data as before. We continue with this process until the number of texts in each subset is smaller than a certain number. At which points the algorithm stops to split the data further. Afterwards, we build many of these decision trees. To obtain a prediction, we let each tree cast a vote and the speaker with the most votes becomes our prediction.

Like before, we can let the computer decide on the minimum number of data required for further splitting and the number of trees in the forest. Moreover, the computer must decide on how many variables the individual trees can choose from at each split. This leaves us with three tuning parameters. As we have over 300 variables to choose from, the training of our model takes a long
time.

Even after talking through an example, the idea of a random forest might still be abstract. After all, there is a lot of randomness involved. Instead, we could think about the process graphically. One way to think about the algorithm is that we build many boxes or splits in which our data points are contained in. As individual tree only looked at some parts of the data, we have many boxes which make different predictions for the same points.

How does that potentially help with the results? In contrast to decision trees, we cannot easily say why we decided on a certain actor and not somebody else. Yet, decision trees are sensitive to the data. For example, what would happen if we randomly selected many texts in which Matt speaks quickly for our training data. A single decision tree would associate this feature with Matt
and base a lot of its predictions on it. This is especially true if different variables are correlated (meaning they move in the same direction). For example, the number of words in a text and time it is spoken over are likely to be correlated. This might negatively affect our predictions. Since a random forest is built using random data and variables, it is less likely to be driven by a high correlation between the features of the text. Thus, our predictions are more protected against the errors of the individual trees.

```{r random_forest, echo = FALSE}
graph %>%
  filter(actor != "New Point") %>%
  ggplot(aes(x, y, color = actor)) +
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77", "#ffa600", "black")) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = c(1, 2, 3, 4, 5, 6),
    limits = c(0, 6.1)
  ) +
  coord_fixed() +
  labs(
    title = "Random Forest Visualized",
    subtitle = "Each Rectangle represents the prediction from a single tree",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  annotate("rect", xmin = 0.5, xmax = 3, ymin = 3.3, ymax = 5.3, alpha = 0.1, fill = "#1b9e77", color = "#1b9e77", size = 0.2) +
  annotate("rect", xmin = 0.7, xmax = 2.8, ymin = 4.85, ymax = 5.2, alpha = 0.1, fill = "#1b9e77", color = "#1b9e77", size = 0.2) +
  annotate("rect", xmin = 4.5, xmax = 5.5, ymin = 2.7, ymax = 3.5, alpha = 0.1, fill = "#1b9e77", color = "#1b9e77", size = 0.2) +
  annotate("rect", xmin = 5.2, xmax = 5.8, ymin = 1.2, ymax = 1.8, alpha = 0.1, fill = "#1b9e77", color = "#1b9e77", size = 0.2) +
  annotate("rect", xmin = 4, xmax = 6.5, ymin = 0, ymax = 3.1, alpha = 0.1, fill = "#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin = 1.6, xmax = 2.5, ymin = 4.2, ymax = 4.7, alpha = 0.1, fill = "#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin = 5, xmax = 6, ymin = 0.5, ymax = 2.5, alpha = 0.1, fill = "#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin = 2, xmax = 6.5, ymin = 1, ymax = 2, alpha = 0.1, fill = "#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin = 2.5, xmax = 3.5, ymin = 4.7, ymax = 5.5, alpha = 0.1, fill = "#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin = 1.6, xmax = 2.5, ymin = 4.2, ymax = 4.7, alpha = 0.1, fill = "#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin = 2, xmax = 3, ymin = 4.5, ymax = 4.8, alpha = 0.1, fill = "#ffa600", color = "#ffa600", size = 0.2) +
  base_theme() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.key = element_blank(),
    legend.text = element_text(color = "#3B3B3B", size = 6),
    legend.background = element_rect(color = NA),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(color = "#656565", size = 0.05),
    axis.text = element_text(color = "#3B3B3B", size = 6),
    axis.title.x = element_text(color = "#3B3B3B", size = 6),
    axis.title.y = element_text(color = "#3B3B3B", size = 6, angle = 90, vjust = 1),
    plot.title.position = "plot",
    plot.title = element_text(size = rel(0.6))
  )
ggsave("./output/images/machine_learning/ml_random_forest_visualized.jpg", width = 4, height = 3)
```

In our final model, 1790 trees are used to make the predictions. At each split, 52 random variables were randomly selected as candidates for the splits. The splitting stops when there are 19 or fewer texts within a split. Unfortunately, the model only manages to rightly assign the right speakers in 21.5 percent of cases. Again, we would do a better job by just predicting Matt for all texts.

## XGBoost

Another approach for extending of decision trees are extremely boosted trees. The XGBoost algorithm tries to improve the performance of a forest towards better prediction systematically. In the first step, the algorithm makes an initial prediction. In our case that could mean that the likelihood that a text is spoken by each speaker is the same (10%). In the second step, it builds a decision tree that minimizes the difference between the initial guess and the actual probability. What does this mean? As every text can only be said by one actor, we can assign a '1' for that person and '0' for everybody else.
Afterwards, we build a decision tree that tries to minimize the difference in our predicted probability (0.2) and the actual probability (0 or 1). To do so, it will compare several random features at each step of the decision tree.

In the third step, we build a new tree using our new predicted probability for each point as the starting point. We continue to build decision tree until, the
computer says it can no longer improve prediction. We can think of this as building a forest, where each new decision tree predicts parts, which were previously unaccounted for. Graphically, the result would look like the random forest.

Again, we let the computer to decide how the best configuration looks like (how many predictors are randomly compared, how big the individual trees are and how many trees are built etc.). In our case, the computer decided that at each step 4 random variables are considered for splitting and 22% of the data is randomly sampled. The minimum number required for further splits is
31 texts. Last, our model contains 176 trees with 6 splits each. The weight given to the prediction of each of these trees is 0.9. Unfortunately, our final XGBoost model only predicts 16.7 percent of the texts correctly.

## (Regularized) Regression

So far, no model gave us satisfactory results and predicting Matt only is still our best approach. Additionally, they also couldn't tell us the effects of the different features of text on the probability of predicting a certain speaker. For
example, we don't know how a talking speed of five words per second influences our prediction that a text is spoken by Matt. One way to estimate these effects, is to use a method called multinomial regression. The model tries to fit a curve through all the points of a certain class. Each curve is than use to make a prediction for the different actors and one with the highest probability is chosen as the prediction for the point. For example, the estimated curve for the time of each segment may look as follows:

```{r regression, echo = FALSE}
graph %>%
  filter(actor != "New Point") %>%
  mutate(y = 1) %>%
  ggplot(aes(x, y, color = actor)) +
  geom_point(size = 0.5) +
  scale_color_manual(values = c("#1b9e77", "#ffa600", "black")) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = c(0, 1),
    limits = c(0, 1.5),
    label = c("0", "100%")
  ) +
  coord_fixed() +
  labs(
    title = "Multinomial Regression Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Probability for Matt and Liam"
  ) +
  geom_curve(aes(xend = 4, yend = 0, x = 1, y = 0), 
             curvature = -0.6, 
             size = 0.1, 
             colour = "#1b9e77") +
  geom_curve(aes(xend = 5.5, yend = 0, x = 2.5, y = 0), 
             curvature = -0.6, 
             size = 0.1, 
             colour = "#ffa600") +
  base_theme() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.key = element_blank(),
    legend.text = element_text(color = "#3B3B3B", size = 6),
    legend.background = element_rect(color = NA),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(color = "#656565", size = 0.05),
    axis.text = element_text(color = "#3B3B3B", size = 6),
    axis.title.x = element_text(color = "#3B3B3B", size = 6),
    axis.title.y = element_text(color = "#3B3B3B", size = 6, angle = 90, vjust = 1),
    plot.title.position = "plot",
    plot.title = element_text(size = rel(0.6))
  )
ggsave("./output/images/machine_learning/ml_regression_visualized.jpg", width = 4, height = 3)
```

Given the curves, we predict Liam for text, which were spoken over less than three seconds and Matt if it lasted longer. Yet, if we were to include other features in the model, this does not have to be the case. Other factors could also decrease the chance for a certain class. For example, the regression could estimate that a text, which contains 'you' has an increase probability to belong to Matt. At the same time, this could also decrease the probability for Liam.

The main benefit of using multinomial regressions is that we obtain estimates for the unbiased effects. However, since the method was developed for the estimation of effects it often serves poorly when used for predictions. This is because it overfits the data, which increases its variance. This means that when we look at different subsets of the data, we may get different values out of our estimation. Thus, our success at predicting speakers will vary quite a lot depending on the subset our estimator was constructed on. Since the method is build for estimation, it uses the full training data to obtain the most robust results. It also does not include any tuning parameters. As we might have
expected the model doesn't do a good job predicting the speaker. It predicts the right speaker in only 19.6 percent of cases. This is better than most of our previous models, but worse than only guessing Matt for each text.

We can improve the performance of multinominal regressions by using its machine learning extension. The regularized regression algorithm adds some bias to increase the predictive power of multinominal regressions. Unfortunately, once we add the bias, we can no longer get unbiased effects. Yet, since we only care about the predictive power of the model this is less of a concern for us.

Visually adding some bias shifts the curves to achieve better results for our predictions. Again, we can use the computer to find the optimal form to add these tuning parameters to the model. For our example, the result may look as follows:

```{r regularized_regression, echo = FALSE}
graph %>%
  filter(actor != "New Point") %>%
  mutate(y = 1) %>%
  ggplot(aes(x, y, color = actor)) +
  geom_point(size = 0.5) +
  scale_color_manual(values = c("#1b9e77", "#ffa600", "black")) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = c(0, 1),
    limits = c(0, 1.5),
    label = c("0", "100%")
  ) +
  coord_fixed() +
  labs(
    title = "Regularized Regression Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Probability for Matt and Liam"
  ) +
  geom_segment(aes(x = 1.3, y = 0.5, xend = 1.1, yend = 0.5),
    arrow = arrow(length = unit(0.05, "cm")),
    color = "#1b9e77",
    size = 0.25
  ) +
  geom_segment(aes(x = 5.2, y = 0.5, xend = 5.4, yend = 0.5),
    arrow = arrow(length = unit(0.05, "cm")),
    color = "#ffa600",
    size = 0.25
  ) +
  geom_curve(aes(xend = 3.5, yend = 0, x = 0.5, y = 0), 
             curvature = -0.6, 
             size = 0.1, 
             colour = "#1b9e77"
             ) +
  geom_curve(aes(xend = 6, yend = 0, x = 3, y = 0), 
             curvature = -0.6, 
             size = 0.1, 
             colour = "#ffa600"
             ) +
  geom_curve(aes(xend = 4, yend = 0, x = 1, y = 0), 
             curvature = -0.6, 
             size = 0.05, 
             colour = "#1b9e77",
             linetype = "dotted") +
  geom_curve(aes(xend = 5.5, yend = 0, x = 2.5, y = 0), 
             curvature = -0.6, 
             size = 0.05, 
             colour = "#ffa600",
             linetype = "dotted") +
  base_theme() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.key = element_blank(),
    legend.text = element_text(color = "#3B3B3B", size = 6),
    legend.background = element_rect(color = NA),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(color = "#656565", size = 0.05),
    axis.text = element_text(color = "#3B3B3B", size = 6),
    axis.title.x = element_text(color = "#3B3B3B", size = 6),
    axis.title.y = element_text(color = "#3B3B3B", size = 6, angle = 90, vjust = 1),
    plot.title.position = "plot",
    plot.title = element_text(size = rel(0.6))
  )
ggsave("./output/images/machine_learning/ml_regularized_regression_visualized.jpg", width = 4, height = 3)
```

Our trained regularized regression manages to predict 23.6 percent of speakers correctly. This is the best performance, which we have achieved so far. Unfortunately, the model is still worse than only predicting Matt for all cases.

## Overview Models

Let's recap what we have achieved so far. We trained to seven different machine learning algorithms to predict the actors of Critical Role. Those seven algorithms had different approaches and assumptions. However, they all served badly overall. In the end, we are still left better of predicting that each
segment is spoken by Matt. This is hardly ideal. Yet, you may recall from the previous blog post that we only included 300 2-and 3-grams in our model. It could be that we achieve better results when we treat the amount of the two n-grams as another tuning parameter.

To save some resources lets only train one last model, with the added flexibility. Yet, which model should we choose as our last one? The obvious pick would be to run another regularized regression as it performed best. We could also train another random forest as it also performed well. However,
training a random forest takes a long time. In contrast, the XGBoost algorithm was designed to be computationally efficient for big data. Moreover, it often performs well in competitions. It could be that we limited our analysis too much for it to perform well.

```{r accuracy_overview, echo = FALSE, warning=FALSE, message = FALSE}
accuracy_overview %>%
  select(model, accuracy) %>%
  filter(model != "xg_boost_tune") %>%
  mutate(
    model = replace(model, model == "decision_tree", "Decision Tree"),
    model = replace(model, model == "regularized", "Regularized Regression"),
    model = replace(model, model == "random_forest", "Random Forest"),
    model = replace(model, model == "knn", "K-nearest Neighbors"),
    model = replace(model, model == "naive_bayes", "Naive Bayes"),
    model = replace(model, model == "xg_boost", "XGboost"),
    model = replace(model, model == "xg_boost_tune", "XGboost Words Tuned"),
    model = replace(model, model == "multinominal", "Multinominal Regression")
  ) %>%
  rename(
    Algorithm = model,
    Accuracy = accuracy
  ) %>%
  ggplot(aes(x = reorder(as.factor(Algorithm), Accuracy), y = Accuracy)) +
  geom_bar(stat = "identity", fill = "#009E73") +
  coord_flip() +
  scale_y_continuous(
    expand = c(0, 0),
    position = "right",
    breaks = c(0.1, 0.2, 0.3),
    limit = c(0, 0.31),
    labels = c(
      "0.1" = "10",
      "0.2" = "20",
      "0.3" = "30%"
    )
  ) +
  labs(
    title = "Overview Performence Algorithms",
    subtitle = "Percent of rightly classified samples.",
  ) +
  bar_chart_theme() +
  theme(
    strip.background = element_rect(fill = "#F0F0F0"),
    panel.grid.major.x = element_line(size = rel(0.4), color = "#656565"),
    axis.line.y = element_line(size = rel(0.4), color = "black"),
    axis.text.x = element_text(color = "black", vjust = 5, size = rel(0.5), alpha(2)),
    axis.text.y = element_text(hjust = 1, color = "black", size = rel(0.75), margin = margin(r = -2)),
    strip.text = element_text(size = 5, face = "bold"),
  )
```

Recall that our initial goal was to differentiate between different actors. Until now, we only looked at the overall performance of the models. Instead, we can also look at how well different models predicted each speaker. It could be that the regularized regression assigns a high probability for Matt. Theoretically, this would mean that the regularized regression is close to guessing Matt for all texts. This wouldn't help us to differentiate between cast
members.

To see how well each model predicts individual speakers, we can look at how many predictions for an actor were spoken by them. This will give us a metrics to judge how well the models learned to identify each cast member. Additionally, we can examine how many text were assigned to the speakers overall. This will tell us how close the model is to predicting Matt for all texts.

In the picture below, we can indeed see that the regularized regression predicts Matt the most (28.6 percent of all cases). However, only 60 percent of the texts it assigns to him were spoken by him. The model predicts that the second most texts belong to Laura. However, the chance that these texts were spoken by her are slim. The model thinks that nearly 15 percent of texts were said by her but is right in only 1.5 percent of cases. This may be unsurprising as Matt and Laura are the ones with the most classified samples in the overall data. Additionally, it performs badly when assigning texts to
Marisha, Liam, Orion, and the guests attending the show. All of which have a probability of being the right actor when predicted by the model of less than five percent. Thus, the regularized regression is indeed best in predicting Matt compared to other speakers. Let's see if the XGBoost model got more balanced results.

```{r regularized_actor, echo = FALSE, warning=FALSE, message = FALSE}
regularized_predictions %>%
  # data
  mutate(
    actor_guest = as.factor(actor_guest),
    .pred_class = as.factor(.pred_class)
  ) %>%
  conf_mat(actor_guest, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  mutate(prediction = ifelse(Prediction == Truth, "rightly classified", "wrongly classified")) %>%
  group_by(Truth, prediction) %>%
  summarise(sum(n)) %>%
  rename(
    count = `sum(n)`,
    actor = Truth
  ) %>%
  ungroup() %>%
  mutate(sum_predictions = sum(count)) %>%
  group_by(actor) %>%
  mutate(percentage_actor = sum(count)) %>%
  mutate(percentage_right = sum(count)) %>%
  ungroup() %>%
  mutate(percentage_actor = (percentage_actor / sum_predictions) * 100) %>%
  mutate(percentage_right = (count / percentage_right) * 100) %>%
  filter(prediction == "rightly classified") %>%
  select(actor, percentage_right, percentage_actor) %>%
  pivot_longer(!actor, names_to = "category", values_to = "percentage") %>%
  # graph
  ggplot(aes(x = reorder(actor, percentage), y = percentage, fill = category)) +
  geom_bar(position = "dodge", stat = "identity") +
  coord_flip() +
  scale_y_continuous(
    expand = c(0, 0),
    position = "right",
    breaks = c(10, 20, 30, 40, 50, 60),
    labels = c("10", "20", "30", "40", "50", "60%")
  ) +
  scale_fill_manual(
    breaks = c("percentage_right", "percentage_actor"),
    values = c("#1b9e77", "#ffa600"),
    labels = c("Percentage of right Predictions if Speaker was predicted", "Percentage of total Predictions")
  ) +
  labs(
    title = "Regularized Regression: Predictions by Actor",
  ) +
  bar_chart_theme() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 5.5),
    legend.title = element_blank(),
    legend.key.height = unit(0.4, "line"),
    legend.key = element_blank(),
    legend.margin = margin(t = 0, unit = "cm")
  )

ggsave("./output/images/machine_learning/ml_regularized_actor.jpg", width = 4, height = 3)
```

Like the regularized regression, the XGBoost model predicts Matt for 28.7 percent of all texts. Unfortunately, it is less successful in assigning the right ones. Only one third of the text actually belong to him. Interestingly, the highest percentage of assigning the right speaker to is for Ashley. Yet, the model only predicts her in 2.6 percent of texts. Moreover, it has a hard time to predict Marisha, Liam, Orion and the guests.

Overall neither model has a convincing performance. Both models only
assign ten percent of text correctly for five actors. These are not impressive numbers. The XGBoost model manages to be correct in twenty percent of cases for three actors compared to two for the regularized regression. This is hardly any hard evidence that the model will be better once we increase the number of words. Intuitively, given that the model was designed for large data sets, it should serve better with more words. Addtionally, it does a better job
differentiating between actors so far. Therefore, we use a XGBoost model, which uses the number of words as a tuning parameter in the following.

```{r xgboost_actor, echo = FALSE, warning=FALSE, message = FALSE}
xgboost_predictions %>%
  # data
  mutate(
    actor_guest = as.factor(actor_guest),
    .pred_class = as.factor(.pred_class)
  ) %>%
  conf_mat(actor_guest, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  mutate(prediction = ifelse(Prediction == Truth, "rightly classified", "wrongly classified")) %>%
  group_by(Truth, prediction) %>%
  summarise(sum(n)) %>%
  rename(
    count = `sum(n)`,
    actor = Truth
  ) %>%
  ungroup() %>%
  mutate(sum_predictions = sum(count)) %>%
  group_by(actor) %>%
  mutate(percentage_actor = sum(count)) %>%
  mutate(percentage_right = sum(count)) %>%
  ungroup() %>%
  mutate(percentage_actor = (percentage_actor / sum_predictions) * 100) %>%
  mutate(percentage_right = (count / percentage_right) * 100) %>%
  filter(prediction == "rightly classified") %>%
  select(actor, percentage_right, percentage_actor) %>%
  pivot_longer(!actor, names_to = "category", values_to = "percentage") %>%
  # graph
  ggplot(aes(x = reorder(actor, percentage), y = percentage, fill = category)) +
  geom_bar(position = "dodge", stat = "identity") +
  coord_flip() +
  scale_y_continuous(
    expand = c(0, 0),
    position = "right",
    limits = c(0,51),
    breaks = seq(10,50,10),
    labels = c("10","20","30","40","50%")
  ) +
  scale_fill_manual(
    breaks = c("percentage_right", "percentage_actor"),
    values = c("#1b9e77", "#ffa600"),
    labels = c("Percentage of right Predictions if Speaker was predicted", "Percentage of total Predictions")
  ) +
  labs(
    title = "XGBoost: Predictions by Actor",
  ) +
  bar_chart_theme() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 5.5),
    legend.title = element_blank(),
    legend.key.height = unit(0.4, "line"),
    legend.key = element_blank(),
    legend.margin = margin(t = 0, unit = "cm")
  )

ggsave("./output/images/machine_learning/ml_xgboost_actor.jpg", width = 4, height = 3)
```

# Number of Words as a Tun**ing
Parameter**

If we let the XGBoost algorithm treat the number of words as a tuning parameter, we can think of this as adding another gearing wheel to our machine. This also means that all other parts of the machine need to be readjusted. In our final model, the algorithm randomly sampled six predictors at each split and required 34 texts for further splitting. 1414 trees were built to make the predictions, each with a maximum depth of 11 splits. It used 46 percent of the training data randomly. Yet, it only used 23 of the most predictive words for its prediction. Intuitively, we would have expected that using more words yields better results. However, the model predicts the right speaker for 22.5 percent of texts. This means that the mThis is still below only
guessing Matt and, more crucially, below the regularized regression.

Looking at the prediction for the individual actors, the results look more like
the ones achieved using the regularized regression. Moreover, only Sam, Liam and the guests are correctly identified in below five percent of cases. In contrast to before, Marisha and Orion place high on the list of correctly identified texts. Yet, the model does not produce very interesting insights.

```{r xgb_tuned, echo = FALSE, warning=FALSE, message = FALSE}
xgb_tune_predictions %>%
  # data
  mutate(
    actor_guest = as.factor(actor_guest),
    .pred_class = as.factor(.pred_class)
  ) %>%
  conf_mat(actor_guest, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  mutate(prediction = ifelse(Prediction == Truth, "rightly classified", "wrongly classified")) %>%
  group_by(Truth, prediction) %>%
  summarise(sum(n)) %>%
  rename(
    count = `sum(n)`,
    actor = Truth
  ) %>%
  ungroup() %>%
  mutate(sum_predictions = sum(count)) %>%
  group_by(actor) %>%
  mutate(percentage_actor = sum(count)) %>%
  mutate(percentage_right = sum(count)) %>%
  ungroup() %>%
  mutate(percentage_actor = (percentage_actor / sum_predictions) * 100) %>%
  mutate(percentage_right = (count / percentage_right) * 100) %>%
  filter(prediction == "rightly classified") %>%
  select(actor, percentage_right, percentage_actor) %>%
  pivot_longer(!actor, names_to = "category", values_to = "percentage") %>%
  # graph
  ggplot(aes(x = reorder(actor, percentage), y = percentage, fill = category)) +
  geom_bar(position = "dodge", stat = "identity") +
  coord_flip() +
  scale_y_continuous(
    expand = c(0, 0),
    position = "right",
    limits = c(0,61),
    breaks = seq(10,60,10),
    labels = c("10","20","30","40","50","60%")
  ) +
  scale_fill_manual(
    breaks = c("percentage_right", "percentage_actor"),
    values = c("#1b9e77", "#ffa600"),
    labels = c("Percentage of right Predictions if Speaker was predicted", "Percentage of total Predictions")
  ) +
  labs(
    title = "XGBoost: Predictions by Actor",
  ) +
  bar_chart_theme() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 5.5),
    legend.title = element_blank(),
    legend.key.height = unit(0.4, "line"),
    legend.key = element_blank(),
    legend.margin = margin(t = 0, unit = "cm")
  )

ggsave("./output/images/machine_learning/ml_xgb_tuned_actor.jpg", width = 4, height = 3)
```

## Conclusion

In summary, we have seen that it is hard to distinguish between the members of Critical Role using the subtitles. We did not achieve any satisfactory results with any of the method we used. Still some caution is warranted. First, in our analysis we used down-sampling to save computational resources. Using up sampling the results might improve. Moreover, we could also give up on the idea of equal representation of the speakers altogether. If we would only
care predictions this approach is likely to achieve better results.

Second, we turned all categories into variables of zeros and ones. This approach is not ideal for using decision trees and might also influence its extensions. Thus, their predictions could potentially be improved by leaving the categories as one variable. Third, we could try to use the predictions from different models and combine them into an ensemble of methods. As we have seen, different models may do a better job at predicting certain speakers than others. Hence, combining their predictions could improve our predictions called ensembles). If we had no computational constraints, we could also train a neural net to potentially achieve better results. Thus, I encourage interested readers to play around with the data themselves to find alternative approaches.
