---
title: "Critical Machine Learning: Prediciting Critical Role Actors from Text. Part 2"
always_allow_html: true
output:
  html_document: default
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_format = "all") })
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
# tell Rmarkdown to run in root folder of project
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

# where figures for rendering the document are saved
knitr::opts_chunk$set(
  fig.path = "markdown_figs/"
)


# packages
library(dplyr) # for data preparation
library(tidyr) # for data preparation
library(stringr) # for data preparation
library(tidytext) # for rearrange facet + bar charts
library(ggplot2) # for graphs
library(kableExtra) # for table
library(purrr) # for pluck command
library(yardstick) # for confusion matrix
library(ggforce) # for drawing circles in the knn-graph
library(data.table) # for faster reading in data
```

```{r themes, include=FALSE}
# graph themes
source("./scripts/themes.R")
```

```{r data, echo = FALSE}
# ------  read in every data frame from the 'machine_learning' folder
path <- "./data/machine_learning/"
# folders
folders_path <- list.files(path = path, pattern = "", full.names = TRUE)
folders_path <- str_replace(folders_path, pattern = "//", replacement = "/")

# extract files and import them
for (i in 1:length(folders_path)) {
  # read in every csv-file in a folder into tibble
  tmp <- folders_path[i]
  files <- list.files(path = paste(tmp), pattern = ".csv")
  files_path <- paste(folders_path[i], "/", files, sep = "")
  file_name <- str_replace(files, ".csv", "")
  myfiles <- lapply(files_path, fread)
  # split tibble into seperate data frames
  for (df in 1:length(myfiles)) {
    tmp <- myfiles[[df]] %>% as.data.frame()
    assign(paste(file_name[df]), tmp)
    rm(tmp)
  }
}
# clean up
rm(df, files, file_name, files_path, folders_path, i, myfiles, path)


# ------  accuracy for all models combined
accuracy_overview <- decision_acc %>%
  bind_rows(., knn_acc) %>%
  bind_rows(., naive_bayes_acc) %>%
  bind_rows(., regularized_acc) %>%
  bind_rows(., rf_acc) %>%
  bind_rows(., xgboost_acc) %>%
  bind_rows(., xgb_tune_acc) %>%
  bind_rows(., multinominal_acc)

rm(decision_acc, knn_acc, naive_bayes_acc, regularized_acc, rf_acc, xgboost_acc, xgb_tune_acc, multinominal_acc)

# ------ time and turns by actor
actor_words_time_percent <- read.csv("./data/descriptive_analysis/actor_words_time.csv")
actor_words_time_percent <- actor_words_time_percent %>%
  select(1, 4, 10) %>%
  pivot_longer(
    cols = c("words_percent", "turns_percent"),
    names_to = "variable",
    values_to = "percent"
  ) %>%
  mutate(
    variable = str_replace(variable, "words_percent", "Words"),
    variable = str_replace(variable, "turns_percent", "Turns")
  )

# add 'just guessing Matt' to accuracy overview
percent_turns_matt <- actor_words_time_percent %>%
  filter(actor_guest == "Matt" & variable == "Turns") %>%
  mutate(percent = percent / 100) %>%
  mutate(
    rightly_classified = round(percent * nrow(rf_predictions)),
    wrongly_classified = (1 - percent) * nrow(rf_predictions)
  )

accuracy_overview <- accuracy_overview %>%
  add_row(
    model = "Only Guessing Matt",
    roc_auc = NA,
    accuracy = percent_turns_matt$percent[1],
    rightly_classified = percent_turns_matt$rightly_classified[1],
    wrongly_classified = percent_turns_matt$wrongly_classified[1]
  )

rm(percent_turns_matt)

# ------ example data
example <- read.csv("./data/descriptive_analysis/example.csv")
example_ml <- example_ml %>%
  select(actor_guest, text, time_in_sec, words_per_minute, arc, segment, combat) %>%
  mutate(segment = str_to_title(segment)) %>%
  rename(
    Actor = actor_guest,
    Text = text,
    `Time in Sec` = time_in_sec,
    `Words per Minute` = words_per_minute,
    Arc = arc,
    Segment = segment,
    Combat = combat
  )

# ------ Cross Validation Graph
nrows <- 10
cross_validation <- expand.grid(y = 1:nrows, x = 1:nrows)

cross_validation <- cross_validation %>%
  mutate(train_test = "Train Data") %>%
  mutate(train_test = replace(train_test, x == y, "Test Data")) %>%
  mutate(train_test = as.factor(train_test))

# ------ Example graph to visualize the algorithms
set.seed(1000)
Liam <- rep(c("Liam"), each = 10)
Liam_x <- rnorm(10, 2, 0.8)
Liam_y <- rnorm(10, 5, 0.7)
Liam_df <- data.frame(
  actor = Liam,
  x = Liam_x,
  y = Liam_y
)
Matt <- rep(c("Matt"), each = 10)
Matt_x <- rnorm(10, 5, 0.5)
Matt_y <- rnorm(10, 2, 0.9)
Matt_df <- data.frame(
  actor = Matt,
  x = Matt_x,
  y = Matt_y
)
graph <- rbind(Liam_df, Matt_df)
# add noise
graph <- graph %>%
  add_row(actor = "Matt", x = 1.8, y = 4.6) %>%
  add_row(actor = "Matt", x = 2.2, y = 4.5) %>%
  add_row(actor = "Liam", x = 5, y = 3) %>%
  add_row(actor = "Liam", x = 5.2, y = 3.2)

# clean up
rm(Liam, Liam_x, Liam_y, Matt, Matt_x, Matt_y, nrows, Matt_df, Liam_df)
```

In the [last post](http://the%20different%20actors%20of%20Critical%20Role%20from%20the%20subtitles.), we discussed how we prepared the Critical Role subtitle data to predict the actors. We split the data into training and testing data sets to train the algorithms. Given the imbalance of the different actors, we down-sampled our training data. Because of computational considerations, we decided to only include the 300 most predictive words in our analysis. In this post we try to predict the actor from the text. We discuss different machine learning algorithms available and how well they can predict the speakers from the text.

## K-Nearest Neighbors

As a first attempt, we can run the 'k-nearest neighbors'-algorithm to predict the speakers. As the name suggests, the algorithm classifies texts by examining the k-nearest points and choosing the speaker who occurs most within them as the prediction. This may be counter-intuitive for our 300 variables. However, we can illustrate the approach by only looking at the number of words of a text and its duration. The graph below plots some imaginary data for two of the actors (Matt and Liam). Let's assume the algorithm makes its prediction based on the 12 nearest points. For point one, this means that more points belong to Liam compared to Matt. Thus, the algorithm predicts that the point belongs to 'Liam' as well. For point two, the opposite applies. Hence, the algorithm predicts 'Matt' as the speaker of the text.

```{r knn_graph, echo = FALSE}
graph %>%
  add_row(actor = "New Point", x = 2, y = 4.5) %>%
  add_row(actor = "New Point", x = 5, y = 2) %>%
  ggplot(aes(x, y, color = actor)) +
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77", "#ffa600", "black")) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = c(1, 2, 3, 4, 5, 6),
    limits = c(0, 6.1)
  ) +
  geom_circle(aes(x0 = 2, y0 = 4.5, r = 1.5), color = "#1b9e77", size = 0.5, inherit.aes = FALSE) +
  geom_circle(aes(x0 = 5, y0 = 2, r = 1.8), color = "#ffa600", size = 0.5, inherit.aes = FALSE) +
  coord_fixed() +
  annotate("text", x = 2.1, y = 4.3, label = "1", size = 1.5, color = "#3B3B3B") +
  annotate("text", x = 4.85, y = 1.85, label = "2", size = 1.5, color = "#3B3B3B") +
  labs(
    title = "12-Nearest Neighbors Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  base_theme() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.key = element_blank(),
    legend.text = element_text(color = "#3B3B3B", size = 6),
    legend.background = element_rect(color = NA),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(color = "#656565", size = 0.05),
    axis.text = element_text(color = "#3B3B3B", size = 6),
    axis.title.x = element_text(color = "#3B3B3B", size = 6),
    axis.title.y = element_text(color = "#3B3B3B", size = 6, angle = 90, vjust = 1),
    plot.title.position = "plot",
    plot.title = element_text(size = rel(0.6))
  )
ggsave("./output/images/machine_learning/ml_knn_visualized.jpg", width = 3, height = 3)
```

In the previous post, we discussed we how could think of machine learning algorithms as machines containing many gearing wheels. Different configurations of the wheels result in different predictions. During training, an algorithm finds the configuration which results in the most accurate predictions.

Which wheels does k-nearest neighbors algorithm adjust during training? One wheel is the optimal number of neighbors it considers for its prediction. For the upper left cluster of points, we can see the two points closest to point one belong to Matt. By only considering the nearest two neighbors, we would alter our prediction for point one to 'Matt'. However, as we can see, all other neighboring points belong to Liam. Thus, only considering the two closest points might mean that chance drives our predictions. Yet, if the model focuses too little on the patterns in the training data, it oversimplifies the model. For example, if the algorithm was to consider the nearest 24 points, it would cannot distinguish between the two clusters of points. In the training process, the k-nearest algorithm finds the optimal number of neighbors. Fortunately, we can let the algorithms decide which number achieves the best predictions during training.

The considerations above highlight one of the key trade-offs in machine learning. If the k-nearest-neighbor algorithm only examines a few neighboring points, its predictions may not generalize well (high variance). If that is the case, we want to increase the number of points. However, there may be some cases in which Matt says the same thing in a short amount of time. One example might be that he asks for the damage done by a player followed by 'How do you want to do so this?'. If the algorithm examines the nearest 11 points for its predictions, these texts will assigned to Liam (high bias). Thus, choosing the right number of neighbors to consider is a case for the bias-variance tradeoff which is present in all machine learning models.

The algorithm also searches for its best configuration for the weighting function. The weighting function determines how the algorithm measures the distance between points. In the example above, we used the direct distance between the points to classify them as neighbors. But other measures are available. Luckily, we can use the computer to determine which weighting function and how many neighboring points yield the best predictions.

After training, our k-nearest neighbor model uses 13 neighbors to make its prediction. To define the nearest neighbors, it uses an inverse distance function. The function gives less weight to points further away from new points. Yet, the total accuracy of the model is not high (0.119). This means we only predict the right speaker for 11.9 percent of texts. As we have seen in the last blog post, the number of texts belonging to Matt is already 28.4 percent. Thus, our model did not help us predict the speaker, and we would be better off predicting Matt for all spoken texts.

In addition, there are two disadvantages to using the k-nearest-neighbors algorithm. First, while the algorithm is fast to train, it takes long to make its predictions. Second, the algorithm does not tell us which features influenced its decision. Taken together, the results are not satisfactory and do not allow for an easy interpretation. Fortunately, we can turn to other algorithms to see if they can help us predict the actors of Critical Role.

## Naive Bayes

One algorithm often used in text classification is Naive Bayes. Its main idea is to calculate the probability that a text belongs to a class (in our case one actor) given the words in the text. It also includes the probability that a text is a class (think of the 28.4 percent of texts from Matt). The model chooses the actor with the highest probability as its prediction of the text. Let's put some flesh on the theoretical bones and look at an example:

> Hello, everyone and good evening to tonight's episode of Critical Role.

In the first step, the algorithm calculates the probability that the text belongs to a certain actor. Recall that we have ten actors and have down-sampled the data, so each actor covers ten percent of the training data. Thus, the algorithm starts with a probability for each actor of 10 percent. In the second step, it adds or subtracts the probability for each actor given the words in the text. Depending on how likely the actors are to use certain words, the probability increases or decreases. As 'tonight's episode of Critical Role' shows the beginning of an episode, it is likely to be said by Matt. Hence, the algorithm should assign the highest probability to him and choose him for its prediction.

The Naive Bayes algorithm has two tuning parameters which it needs to configure during training. It needs to account for features which happen relatively rarely. It must do so since only a few actors only said certain words and not adjusting the count for them will cause probabilities of zero or one for these speakers. As it is not realistic that a certain actor speaks a text with 100 or 0 percent probability, the counts of these low-frequency words need to be adjusted. To do so, the algorithm adds a small amount to the count of all variables to ensure realistic probabilities.

The computer also needs to decide how smooth class boundaries should be. What does that mean? In contrast to the k-nearest neighbors algorithm, our boundaries for the predictions can be smooth (see picture below). The smoothness of the boundaries influences the predictions. If we make the boundaries too smooth, we could exclude the two points for Liam in the bottom right cluster and include them in the Matt one. However, as we don't know whether this helps our predictions, we let the computer tune it for us.

```{r naive_bayes, echo = FALSE}
graph %>%
  ggplot(aes(x, y, color = actor)) +
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77", "#ffa600", "black")) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = c(1, 2, 3, 4, 5, 6),
    limits = c(0, 6.1)
  ) +
  geom_curve(aes(xend = 3, yend = 0, x = 7, y = 5), curvature = 0.5, size = 0.1,
             linetype = 2, colour = "#3B3B3B") +
    annotate("text", x = 2.5, y = 5.8, label = "Liam", size = 1.8, color = "#3B3B3B") +
  annotate("text", x = 6.5, y = 3.5, label = "Matt", size = 1.8, color = "#3B3B3B") +
  coord_fixed() +
  labs(
    title = "Naive Bayes Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  base_theme() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.key = element_blank(),
    legend.text = element_text(color = "#3B3B3B", size = 6),
    legend.background = element_rect(color = NA),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(color = "#656565", size = 0.05),
    axis.text = element_text(color = "#3B3B3B", size = 6),
    axis.title.x = element_text(color = "#3B3B3B", size = 6),
    axis.title.y = element_text(color = "#3B3B3B", size = 6, angle = 90, vjust = 1),
    plot.title.position = "plot",
    plot.title = element_text(size = rel(0.6))
  )
ggsave("./output/images/machine_learning/ml_naive_bayes_visualized.jpg", width = 3, height = 3)
```

To get its prediction, the Naive Bayes algorithm needs to assume that all our 300 features are independent. You can think of this as assuming that all words are equally important. While this assumption may often be unreasonable, it's widely used in text classification. Yet, it is not ideal to have numeric features like the talking speed in our analysis. But in the interest of comparability, we include these features as we have done so far. If Naive Bayes yields good predictions compared to other models, we can rerun it with a better setup.

In our trained Naive Bayes model, the computer decided on a smoothness of 1.211 and adds 2.88 to the count of each feature. The former tells us how smooth the boundaries are (the line in the picture above). However, they are not easily interpretable. Yet the model only achieves an accuracy of 0.03. This means that the algorithm only predicts the right speaker in three percent of cases. This is a decrease compared to the nearest neighbor model and simply guessing Matt for every text is still our best approach.

Neither of the two models we examined so far allow an intuitive interpretation. Wouldn't it be nice to have some simple if-then rules without knowing all the mathematical shenanigans? Luckily, the decision tree algorithm gives us a possibility to get just that. Among other applications, banks use them to explain to clients why they didn't get a credit. Let's build a decision tree to see which features are relevant for the predictions.

## Decision Tree

Decision Trees are the most illustrative example of a machine learning. After training the model, we get several if-then statements which guide the predictions. Let's look at how this would look like for our example for differentiating between Matt and Liam. In very simple terms, we could make the simple rule to say: if the duration of a text is longer than three seconds, we classify it as spoken by 'Matt'. If it is shorter than three seconds, we classify it as 'Liam'. Applying this simple rule, we only miss classify the two familiar points for each speaker (see picture below).

```{r decision_tree_one_line, echo = FALSE}
graph %>%
  filter(actor != "New Point") %>%
  ggplot(aes(x, y, color = actor)) +
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77", "#ffa600", "black")) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = c(1, 2, 3, 4, 5, 6),
    limits = c(0, 6.1)
  ) +
  coord_fixed() +
  geom_vline(
    xintercept = 3,
    size = 0.15,
    color = "#3B3B3B",
    linetype = "dashed"
  ) +
  annotate("text", x = 2.5, y = 5.8, label = "Liam", size = 1.8, color = "#3B3B3B") +
  annotate("text", x = 3.5, y = 5.8, label = "Matt", size = 1.8, color = "#3B3B3B") +
  labs(
    title = "Decision Tree Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  base_theme() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.key = element_blank(),
    legend.text = element_text(color = "#3B3B3B", size = 6),
    legend.background = element_rect(color = NA),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(color = "#656565", size = 0.05),
    axis.text = element_text(color = "#3B3B3B", size = 6),
    axis.title.x = element_text(color = "#3B3B3B", size = 6),
    axis.title.y = element_text(color = "#3B3B3B", size = 6, angle = 90, vjust = 1),
    plot.title.position = "plot",
    plot.title = element_text(size = rel(0.6))
  )
ggsave("./output/images/machine_learning/ml_decision_tree_visualized_one.jpg", width = 3, height = 3)
```

Yet, the decision tree can come up with more nuanced rules. It could first decide on our proposed rule. Afterwards, it could say 'if a text is longer than three seconds and the talking speed is three words per seconds or faster' predict Liam. Using the new rule, we only wrongly assign the two points in the top left cluster to Matt. The picture below visualizes the rule.

```{r decision_tree_two_lines, echo = FALSE}
graph %>%
  filter(actor != "New Point") %>%
  ggplot(aes(x, y, color = actor)) +
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77", "#ffa600", "black")) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = c(1, 2, 3, 4, 5, 6),
    limits = c(0, 6.1)
  ) +
  coord_fixed() +
  geom_vline(
    xintercept = 3,
    size = 0.15,
    color = "#3B3B3B",
    linetype = "dashed"
  ) +
  geom_curve(aes(x = 3, y = 2.96, xend = 7, yend = 2.96), curvature = 0, size = 0.12, color = "#3B3B3B", linetype = "dashed") +
  annotate("text", x = 2.5, y = 5.8, label = "Liam", size = 1.8, color = "#3B3B3B") +
  annotate("text", x = 3.5, y = 5.8, label = "Liam", size = 1.8, color = "#3B3B3B") +
  annotate("text", x = 3.4, y = 2.8, label = "Matt", size = 1.8, color = "#3B3B3B") +
  annotate("text", x = 3.4, y = 4.5, label = "1.", size = 2, color = "#3B3B3B") +
  annotate("text", x =7, y = 3.5, label = "2.", size = 2, color = "#3B3B3B") +
  labs(
    title = "Decision Tree Visualized (Continued)",
    subtitle = "",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  base_theme() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.key = element_blank(),
    legend.text = element_text(color = "#3B3B3B", size = 6),
    legend.background = element_rect(color = NA),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(color = "#656565", size = 0.05),
    axis.text = element_text(color = "#3B3B3B", size = 6),
    axis.title.x = element_text(color = "#3B3B3B", size = 6),
    axis.title.y = element_text(color = "#3B3B3B", size = 6, angle = 90, vjust = 1),
    plot.title.position = "plot",
    plot.title = element_text(size = rel(0.6))
  )
ggsave("./output/images/machine_learning/ml_decision_tree_visualized_two.jpg", width = 3, height = 3)
```

How could we turn our naÃ¯ve approach into a more systematic one? To decide how to split the data, the decision tree algorithm tests all potential splits and examines which one achieves the best differentiation between actors. For our example, the algorithm checks whether splitting at 1 seconds or 2 seconds (or more) would be best to distinguish between Matt and Liam. Likewise, it checks for potential splits for the number of words per seconds. Looking at the picture above, we can see that splitting the data at three words per seconds differentiates best between the two actors. Thus, the first split from the tree would be at a talking speed of three words per second.

In the next step, the algorithm tries to split each side again to improve its predictions. If we look at the bottom part of the graph, we see that it only contains point which belong to Matt. Therefore, we do not split it any further. However, there are two points in the top half of the graph which the algorithm attributes to Liam, which belong to Matt. Unfortunately, there is no single split which could help us capture both points, as all splits result in most points belonging to Liam. Therefore, the algorithm would not split the data any further.

It seems unlikely that we can always separate different speakers as nicely as in our example. Even if we could, it may not be a good idea to do so (since it may cause over-fitting and a high variance). So how does a decision tree decide on when to stop divide the data further? We could say that the number of points we need to have in each split should be at least a certain number. In our case, we might say that making a prediction based on ten points (the number of points in the bottom right cluster) is too uncertain for us. Instead, we might prefer to split the data at a duration of four seconds as it leaves twelve points in each segment. Likewise, we could simply say that we like to split our data only a certain number of times. Alternatively, we can tell the algorithm that we prefer fewer splits over too many. We achieve this by adding a complexity penalty for each split to the algorithm. Like before, since we don't know which configuration works best, we let the decision tree algorithm tune itself.

Our trained decision tree is seven splits long. To consider further splits, it required a minimum of 11 points. For each split, it induces a penalty of 4.43 wrongly classified speakers. Unfortunately, a tree with seven decisions is too big to be shown here (but is available on [GitHub](https://github.com/datengefluester/Critical-Role-NLP/blob/main/data/machine_learning/decision-tree/decision_tree.png)). However, we can look at one path to illustrate it. The first question the decision tree model asks is whether the text has over 133 characters (1). If that is the case, the model looks at whether there are less than 248 lower characters in the text (2). In case this applies as well, the model examines whether the text contains the words 'know' (3) 'going' (4) 'use' (5) and 'oh' (6). When the text includes all words, it asks whether the text additionally contains the word 'just' (7). If it does, the algorithm predicts that 'Sam' speaks the text. If it does not, it predicts 'Matt'. Other rules contain more text specific features, like the number of periods in a text.

|                                                                                                                                                                                                                                            |            |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|
| Text                                                                                                                                                                                                                                       | Prediction |
| and with that, you know what? it's going to go ahead and do this. the entity then moves backwards. pike and scanlan oh, you already used your reaction to do cutting words, so you don't get that. pike, go ahead and make a melee attack. | Matt       |
| oh, well, we don't need to keep saying these things. you know, if you guys would like to stay just for a few days, because we're going to be leaving.                                                                                      | Sam        |

The decision tree assigns 20.7 percent of texts to the right speaker. This is an enormous improvement over our two previous models. Still, it is worse than predicting only Matt for all our texts. Fortunately, there are other methods available which extend the decision tree algorithm. Let's see if we can use them to improve our predictions.

## Random Forest

One extension of decisions trees are random forests. As the name suggests, the algorithm builds many decision trees randomly. In the first step, it randomly selects parts of our training data (a process called bagging). The algorithm bootstraps the data, meaning that it can choose each point more than once. In the second step, it builds a decision tree from the new data set but only chooses several random variables as candidates for each split. After each split, the algorithm chooses another set of random variables and chooses the best one for the next split. It then builds many of these decisions trees to create a forest. To make a prediction, the algorithm lets all individual trees cast a vote. The actor with the most votes becomes the prediction of the new point.

As the procedure is abstract, let's talk about what the process would mean for our Liam-versus-Matt example. First, we build our new data set, which might contain copies of the same dialogue. Afterwards, we build our first tree. To do so, we randomly select two variables and choose the split that best distinguishes between the two actors. Let's say we selected the number of characters in a text and whether a text happens during combat. We might conclude that splitting the data at 120 characters does the best job of identifying both actors. Hence, we split the data if it contains over 120 characters.

After splitting the data, we again choose two random variables. Now we could decide whether the words per minute or the number of commas in a text improves our predictions the most. After we reach a decision on the best split, we split the data as before. We continue with this process until the number of texts in each subset is smaller than a certain number. Afterwards, we build many of these decision trees. To get a prediction, we let each tree cast a vote and the speaker with the most votes becomes our prediction.

Like before, we can let the computer decide on the minimum number of data required for further splitting and the number of trees in the forest. The computer must also decide how many variables the individual trees can choose from at each split. This leaves us with three gearing wheels to find the best configuration for. As we have over 300 variables to choose from, the training of our model takes a long time.

Even after talking through an example, the idea of a random forest might still be abstract. After all, there is a lot of randomness involved. Instead, we could think about the process graphically. One way to think about the algorithm is that we build many boxes or splits which contains data points. As one individual tree only looked at some parts of the data, we have many boxes which make different predictions for the same points.

How does that potentially help with the results? In contrast to decision trees, we cannot easily say why we decided on a certain actor and not somebody else. Yet decision trees are sensitive to the data. For example, what would happen if we randomly selected many texts in which Matt speaks quickly for our training data. A single decision tree would associate this feature with Matt and base a lot of its predictions on this feature. This is especially true if they correlate different variables (meaning they move in the same direction). Among others, the number of words and the duration of a text are likely to be correlated. This might negatively affect our predictions. Since random forests built their trees using random data and variables, they are less likely to be driven by a high correlation between the features of the text. Thus, our predictions are more protected against the errors of the individual trees.

```{r random_forest, echo = FALSE}
graph %>%
  filter(actor != "New Point") %>%
  ggplot(aes(x, y, color = actor)) +
  geom_point(size = 0.4) +
  scale_color_manual(values = c("#1b9e77", "#ffa600", "black")) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = c(1, 2, 3, 4, 5, 6),
    limits = c(0, 6.1)
  ) +
  coord_fixed() +
  labs(
    title = "Random Forest Visualized",
    subtitle = "Each Rectangle represents the prediction from a single tree",
    x = "Time in Second",
    y = "Words per Second"
  ) +
  annotate("rect", xmin = 0.5, xmax = 3, ymin = 3.3, ymax = 5.3, alpha = 0.1, fill = "#1b9e77", color = "#1b9e77", size = 0.2) +
  annotate("rect", xmin = 0.7, xmax = 2.8, ymin = 4.85, ymax = 5.2, alpha = 0.1, fill = "#1b9e77", color = "#1b9e77", size = 0.2) +
  annotate("rect", xmin = 4.5, xmax = 5.5, ymin = 2.7, ymax = 3.5, alpha = 0.1, fill = "#1b9e77", color = "#1b9e77", size = 0.2) +
  annotate("rect", xmin = 5.2, xmax = 5.8, ymin = 1.2, ymax = 1.8, alpha = 0.1, fill = "#1b9e77", color = "#1b9e77", size = 0.2) +
  annotate("rect", xmin = 4, xmax = 6.5, ymin = 0, ymax = 3.1, alpha = 0.1, fill = "#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin = 1.6, xmax = 2.5, ymin = 4.2, ymax = 4.7, alpha = 0.1, fill = "#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin = 5, xmax = 6, ymin = 0.5, ymax = 2.5, alpha = 0.1, fill = "#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin = 2, xmax = 6.5, ymin = 1, ymax = 2, alpha = 0.1, fill = "#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin = 2.5, xmax = 3.5, ymin = 4.7, ymax = 5.5, alpha = 0.1, fill = "#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin = 1.6, xmax = 2.5, ymin = 4.2, ymax = 4.7, alpha = 0.1, fill = "#ffa600", color = "#ffa600", size = 0.2) +
  annotate("rect", xmin = 2, xmax = 3, ymin = 4.5, ymax = 4.8, alpha = 0.1, fill = "#ffa600", color = "#ffa600", size = 0.2) +
  base_theme() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.key = element_blank(),
    legend.text = element_text(color = "#3B3B3B", size = 6),
    legend.background = element_rect(color = NA),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(color = "#656565", size = 0.05),
    axis.text = element_text(color = "#3B3B3B", size = 6),
    axis.title.x = element_text(color = "#3B3B3B", size = 6),
    axis.title.y = element_text(color = "#3B3B3B", size = 6, angle = 90, vjust = 1),
    plot.title.position = "plot",
    plot.title = element_text(size = rel(0.6))
  )
ggsave("./output/images/machine_learning/ml_random_forest_visualized.jpg", width = 3, height = 3)
```

In our trained decision tree, 1790 trees are used to make the predictions. At each split, the algorithm selected 52 random variables as candidates for the splits. The splitting stops when there are 19 or fewer texts within a split. Unfortunately, the model only assigns the right speakers in 21.5 percent of cases. Again, we would do a better job by just predicting Matt for all texts.

## **Extremely Boosted Trees**

Extremely Boosted Trees are another approach for extending decision trees. The XGBoost algorithm tries to improve the performance of a forest towards better prediction systematically. In the first step, the algorithm makes an initial prediction. That could mean that the likelihood that an actor says a text is the same (10%). In the second step, it builds a decision tree that minimizes the difference between the initial guess and the actual probability. What does this mean? As one actor can only say every text, we can assign a '1' for that person and '0' for everybody else. Afterwards, we build a decision tree that tries to minimize the difference in our predicted probability (0.1) and the actual probability (0 or 1). To do so, the XGBoost algorithm, like the random forest, will compare random data and random features at each step of the decision tree.

In the third step, we build a new tree using our new predicted probability for each point as the starting point. We continue to build decision tree until the computer says it can no longer improve its prediction. During training, the algorithm also needs to determine how much weight to give to each tree. We can think of the algorithm as building a forest where each new tree predicts parts which were previously unaccounted for. Graphically, the result would look like the random forest because of the randomness involved.

For our final XGBoost model, the algorithm considered four random variables for splitting and uses 22% of the training data. The minimum number required for further splits is 31 texts. Our model contains 176 trees with 6 splits each. The weight given to the prediction of each of these trees is 0.9. Unfortunately, our final XGBoost model only predicts 16.7 percent of the texts correctly. We will discuss reasons for the poor performance later on.

## (Regularized) Regression

So far, no model gave us satisfactory results and predicting Matt only is still our best approach. They also couldn't tell us the effects of distinct features of text on the probability of predicting a certain speaker. For example, we don't know how a talking speed of five words per second influences our prediction that Matt speaks a text. One way to estimate these effects is to use a method called multinomial regression. The model tries to fit a curve through all the points of a certain class. Each curve is then used to make a prediction for the different actors and the one with the highest probability becomes the prediction for the point. For example, the estimated curve for the time of each segment may look as follows:

```{r regression, echo = FALSE}
graph %>%
  filter(actor != "New Point") %>%
  mutate(y = 1) %>%
  ggplot(aes(x, y, color = actor)) +
  geom_point(size = 0.5) +
  scale_color_manual(values = c("#1b9e77", "#ffa600", "black")) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = c(0, 1),
    limits = c(0, 1.5),
    label = c("0", "100%")
  ) +
  coord_fixed() +
  labs(
    title = "Multinomial Regression Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Probability for Matt and Liam"
  ) +
  geom_curve(aes(xend = 4, yend = 0, x = 1, y = 0), 
             curvature = -0.6, 
             size = 0.1, 
             colour = "#1b9e77") +
  geom_curve(aes(xend = 5.5, yend = 0, x = 2.5, y = 0), 
             curvature = -0.6, 
             size = 0.1, 
             colour = "#ffa600") +
  base_theme() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.key = element_blank(),
    legend.text = element_text(color = "#3B3B3B", size = 6),
    legend.background = element_rect(color = NA),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(color = "#656565", size = 0.05),
    axis.text = element_text(color = "#3B3B3B", size = 6),
    axis.title.x = element_text(color = "#3B3B3B", size = 6),
    axis.title.y = element_text(color = "#3B3B3B", size = 6, angle = 90, vjust = 1),
    plot.title.position = "plot",
    plot.title = element_text(size = rel(0.6))
  )
ggsave("./output/images/machine_learning/ml_regression_visualized.jpg", width = 4, height = 2.5)
```

Given the curves, we predict Liam for texts which were spoken over less than three seconds. If it lasted longer, we predict Matt. Yet, if we were to include other features in the model, this does not have to be the case. Other factors could also decrease the chance for a certain class. For example, the regression could estimate that a text which contains 'you' has an increased probability of belonging to Matt. This could also decrease the probability of Liam.

The major benefit of using multinomial regressions is that we obtain estimates for the effects of distinct features. However, since the method was developed for the estimation of effects, it often serves poorly when used for predictions. This is because it overfits the data to get unbiased results, which increases its variance. It also does not include any tuning parameters. As we might have expected, the model doesn't do a good job predicting the speaker. It predicts the right speaker in only 19.6 percent of cases. This is better than most of our previous models, but worse than only guessing Matt for each text.

We can improve the performance of multinominal regressions by using its machine learning extension. The regularized regression algorithm adds some bias to increase the predictive power of multinominal regressions. Unfortunately, once we add the bias, we can no longer get unbiased effects. Yet, since we only care about the predictive power of the model, this is less of a concern for us.

Adding some bias to the regression shifts the curves to achieve better results for our predictions. Again, we can use the computer to find the optimal form to add these tuning parameters to the model. For our example, the result may look as follows:

```{r regularized_regression, echo = FALSE}
graph %>%
  filter(actor != "New Point") %>%
  mutate(y = 1) %>%
  ggplot(aes(x, y, color = actor)) +
  geom_point(size = 0.5) +
  scale_color_manual(values = c("#1b9e77", "#ffa600", "black")) +
  scale_y_continuous(
    expand = c(0, 0),
    breaks = c(0, 1),
    limits = c(0, 1.5),
    label = c("0", "100%")
  ) +
  coord_fixed() +
  labs(
    title = "Regularized Regression Visualized",
    subtitle = "",
    x = "Time in Second",
    y = "Probability for Matt and Liam"
  ) +
  geom_segment(aes(x = 1.3, y = 0.5, xend = 1.1, yend = 0.5),
    arrow = arrow(length = unit(0.05, "cm")),
    color = "#1b9e77",
    size = 0.25
  ) +
  geom_segment(aes(x = 5.2, y = 0.5, xend = 5.4, yend = 0.5),
    arrow = arrow(length = unit(0.05, "cm")),
    color = "#ffa600",
    size = 0.25
  ) +
  geom_curve(aes(xend = 3.5, yend = 0, x = 0.5, y = 0), 
             curvature = -0.6, 
             size = 0.1, 
             colour = "#1b9e77"
             ) +
  geom_curve(aes(xend = 6, yend = 0, x = 3, y = 0), 
             curvature = -0.6, 
             size = 0.1, 
             colour = "#ffa600"
             ) +
  geom_curve(aes(xend = 4, yend = 0, x = 1, y = 0), 
             curvature = -0.6, 
             size = 0.05, 
             colour = "#1b9e77",
             linetype = "dotted") +
  geom_curve(aes(xend = 5.5, yend = 0, x = 2.5, y = 0), 
             curvature = -0.6, 
             size = 0.05, 
             colour = "#ffa600",
             linetype = "dotted") +
  base_theme() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.key = element_blank(),
    legend.text = element_text(color = "#3B3B3B", size = 6),
    legend.background = element_rect(color = NA),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(color = "#656565", size = 0.05),
    axis.text = element_text(color = "#3B3B3B", size = 6),
    axis.title.x = element_text(color = "#3B3B3B", size = 6),
    axis.title.y = element_text(color = "#3B3B3B", size = 6, angle = 90, vjust = 1),
    plot.title.position = "plot",
    plot.title = element_text(size = rel(0.6))
  )
ggsave("./output/images/machine_learning/ml_regularized_regression_visualized.jpg", width = 4, height = 2.5)
```

Our trained regularized regression predicts 23.6 percent of speakers correctly. This is the best performance which we have achieved so far. Unfortunately, the model is still worse than only predicting Matt for all cases.

## **Comparisons of the different Model**

Let's recap what we have achieved so far. We trained seven different machine learning algorithms to predict the actors of Critical Role. Those seven algorithms had different approaches and assumptions. However, they all served badly overall. In the end, our best approach is to predict Matt for each text. This is hardly ideal. Yet you may recall from the previous blog post that we only included 300 combinations of words in our model. Maybe we achieve better results when we let the algorithms decide on the number themselves.

To save some resources, let's only train one last model with the added flexibility. Which model should we choose as our last one? The obvious pick would be to run another regularized regression as it performed best. In contrast, the XGBoost algorithm is computationally efficient for big-data sets. We could have limited our analysis too much for it to achieve good predictions. As training a random forest takes a long time, we do not consider it in the following.

```{r accuracy_overview, echo = FALSE, warning=FALSE, message = FALSE}
accuracy_overview %>%
  select(model, accuracy) %>%
  filter(model != "xg_boost_tune") %>%
  mutate(
    model = replace(model, model == "decision_tree", "Decision Tree"),
    model = replace(model, model == "regularized", "Regularized Regression"),
    model = replace(model, model == "random_forest", "Random Forest"),
    model = replace(model, model == "knn", "K-nearest Neighbors"),
    model = replace(model, model == "naive_bayes", "Naive Bayes"),
    model = replace(model, model == "xg_boost", "XGboost"),
    model = replace(model, model == "xg_boost_tune", "XGboost Words Tuned"),
    model = replace(model, model == "multinominal", "Multinominal Regression")
  ) %>%
  rename(
    Algorithm = model,
    Accuracy = accuracy
  ) %>%
  ggplot(aes(x = reorder(as.factor(Algorithm), Accuracy), y = Accuracy)) +
  geom_bar(stat = "identity", fill = "#009E73") +
  coord_flip() +
  scale_y_continuous(
    expand = c(0, 0),
    position = "right",
    breaks = c(0.1, 0.2, 0.3),
    limit = c(0, 0.31),
    labels = c(
      "0.1" = "10",
      "0.2" = "20",
      "0.3" = "30%"
    )
  ) +
  labs(
    title = "Overview Accuracy Algorithms",
    subtitle = "Percent of rightly classified samples.",
  ) +
  bar_chart_theme() +
  theme(
    strip.background = element_rect(fill = "#F0F0F0"),
    panel.grid.major.x = element_line(size = rel(0.4), color = "#656565"),
    axis.line.y = element_line(size = rel(0.4), color = "black"),
    axis.text.x = element_text(color = "black", vjust = 5, size = rel(0.5), alpha(2)),
    axis.text.y = element_text(hjust = 1, color = "black", size = rel(0.75), margin = margin(r = -2)),
    strip.text = element_text(size = 5, face = "bold"),
  )
ggsave("./output/images/machine_learning/accuracy_models.jpg", width = 4, height = 3)

```

Recall that our initial goal was to differentiate between different actors. Until now, we only looked at the overall performance of the models. Instead, we can look at how well different models predicted each speaker. Maybe the regularized regression assigns a high probability for Matt. Theoretically, this would mean that the regularized regression is close to guessing Matt for all texts.

To see how well each model predicts individual speakers, we can look at how many predictions for each actor were correct. This will give us a metrics to judge how well the models learned to identify each cast member. We can also examine how many texts they assigned to the speakers overall. This tells us how close the model is to predicting Matt for all texts.

In the picture below, we can indeed see that the regularized regression predicts Matt the most (28.6 percent of all cases). However, he only actually said 60 percent of the texts the algorithm assigned to him. The model performs badly when assigning texts to Laura, Marisha, Liam, Orion and the guests attending the show. All of which have a probability of being the right actor when predicted by the model of less than five percent. Thus, the regularized regression is indeed best in predicting Matt compared to other speakers. Let's see if the XGBoost model got a more balanced result.

```{r regularized_actor, echo = FALSE, warning=FALSE, message = FALSE}
regularized_predictions %>%
  # data
  mutate(
    actor_guest = as.factor(actor_guest),
    .pred_class = as.factor(.pred_class)
  ) %>%
  conf_mat(actor_guest, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  mutate(prediction = ifelse(Prediction == Truth, "rightly classified", "wrongly classified")) %>%
  group_by(Truth, prediction) %>%
  summarise(sum(n)) %>%
  rename(
    count = `sum(n)`,
    actor = Truth
  ) %>%
  ungroup() %>%
  mutate(sum_predictions = sum(count)) %>%
  group_by(actor) %>%
  mutate(percentage_actor = sum(count)) %>%
  mutate(percentage_right = sum(count)) %>%
  ungroup() %>%
  mutate(percentage_actor = (percentage_actor / sum_predictions) * 100) %>%
  mutate(percentage_right = (count / percentage_right) * 100) %>%
  filter(prediction == "rightly classified") %>%
  select(actor, percentage_right, percentage_actor) %>%
  pivot_longer(!actor, names_to = "category", values_to = "percentage") %>%
  # graph
  ggplot(aes(x = reorder(actor, percentage), y = percentage, fill = category)) +
  geom_bar(position = "dodge", stat = "identity") +
  coord_flip() +
  scale_y_continuous(
    expand = c(0, 0),
    position = "right",
    breaks = c(10, 20, 30, 40, 50, 60),
    labels = c("10", "20", "30", "40", "50", "60%")
  ) +
  scale_fill_manual(
    breaks = c("percentage_actor","percentage_right"),
    values = c("#1b9e77", "#ffa600"),
    labels = c("Percentage of right Predictions if Speaker was predicted", "Percentage of total Predictions")
  ) +
  labs(
    title = "Regularized Regression: Predictions by Actor",
  ) +
  guides(fill=guide_legend(nrow=2,byrow=TRUE)) + 
  bar_chart_theme() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 5.5),
    legend.title = element_blank(),
    legend.key.height = unit(0.4, "line"),
    legend.key = element_blank(),
    legend.margin = margin(t = 0, unit = "cm"),
  )

ggsave("./output/images/machine_learning/ml_regularized_actor.jpg", width = 4, height = 3)
```

Like the regularized regression, the XGBoost model predicts Matt for 28.7 percent of all texts. Unfortunately, it is less successful in assigning the right ones. Only one third of the predicted texts actually belong to him. Interestingly, Ashley has the highest percentage of getting the right texts assigned to. Yet the model only predicts her in 2.6 percent of texts. Like the regularized regression, it has a hard time to predict Marisha Liam Orion and the guests.

Overall, neither model has a convincing performance. Both models only assign ten percent of text correctly for five actors. These are not impressive numbers. The XGBoost model is correct in twenty percent of cases for three actors compared to two for the regularized regression. This is hardly any hard evidence that the model will be better once we increase the number of words. Intuitively, given that the algorithm was designed for large data sets, it should serve better with more words. So far, it also does a better job differentiating between actors. Therefore, we use a XGBoost model which uses the number of words as a tuning parameter in the following.

```{r xgboost_actor, echo = FALSE, warning=FALSE, message = FALSE}
xgboost_predictions %>%
  # data
  mutate(
    actor_guest = as.factor(actor_guest),
    .pred_class = as.factor(.pred_class)
  ) %>%
  conf_mat(actor_guest, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  mutate(prediction = ifelse(Prediction == Truth, "rightly classified", "wrongly classified")) %>%
  group_by(Truth, prediction) %>%
  summarise(sum(n)) %>%
  rename(
    count = `sum(n)`,
    actor = Truth
  ) %>%
  ungroup() %>%
  mutate(sum_predictions = sum(count)) %>%
  group_by(actor) %>%
  mutate(percentage_actor = sum(count)) %>%
  mutate(percentage_right = sum(count)) %>%
  ungroup() %>%
  mutate(percentage_actor = (percentage_actor / sum_predictions) * 100) %>%
  mutate(percentage_right = (count / percentage_right) * 100) %>%
  filter(prediction == "rightly classified") %>%
  select(actor, percentage_right, percentage_actor) %>%
  pivot_longer(!actor, names_to = "category", values_to = "percentage") %>%
  # graph
  ggplot(aes(x = reorder(actor, percentage), y = percentage, fill = category)) +
  geom_bar(position = "dodge", stat = "identity") +
  coord_flip() +
  scale_y_continuous(
    expand = c(0, 0),
    position = "right",
    limits = c(0,51),
    breaks = seq(10,50,10),
    labels = c("10","20","30","40","50%")
  ) +
  scale_fill_manual(
    breaks = c("percentage_actor","percentage_right"),
    values = c("#1b9e77", "#ffa600"),
    labels = c("Percentage of right Predictions if Speaker was predicted", "Percentage of total Predictions")
  ) +
  labs(
    title = "XGBoost: Predictions by Actor",
  ) +
  guides(fill=guide_legend(nrow=2,byrow=TRUE)) + 
  bar_chart_theme() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 5.5),
    legend.title = element_blank(),
    legend.key.height = unit(0.4, "line"),
    legend.key = element_blank(),
    legend.margin = margin(t = 0, unit = "cm")
  )

ggsave("./output/images/machine_learning/ml_xgboost_actor.jpg", width = 4, height = 3)
```

## XGBoost with the Number of Words as a Tuning Parameter

We can think of allowing a flexible number of words into a model as another gearing wheel to our machine. This means that all other parts of the machine need to be readjusted. In our last model, the XGBoost algorithm randomly sampled six predictors at each split and required 34 texts for further splitting. It built 1414 trees to make the predictions, each with a maximum depth of 11 splits. It used 46 percent of the training data randomly. Yet, it only used 23 of the most predictive words for its prediction. Intuitively, we would have expected that using more words yields better results. However, the model predicts the right speaker for 22.5 percent of texts. This is still below only guessing Matt and more crucially below the regularized regression.

Looking at the prediction for the individual actors, the results look more like the regularized regression. Only Sam, Liam and the guests are correctly identified in below five percent of cases. In contrast to before, Marisha and Orion place higher on the list of correctly identified texts. Yet the model does not produce very interesting insights.

```{r xgb_tuned, echo = FALSE, warning=FALSE, message = FALSE}
xgb_tune_predictions %>%
  # data
  mutate(
    actor_guest = as.factor(actor_guest),
    .pred_class = as.factor(.pred_class)
  ) %>%
  conf_mat(actor_guest, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  mutate(prediction = ifelse(Prediction == Truth, "rightly classified", "wrongly classified")) %>%
  group_by(Truth, prediction) %>%
  summarise(sum(n)) %>%
  rename(
    count = `sum(n)`,
    actor = Truth
  ) %>%
  ungroup() %>%
  mutate(sum_predictions = sum(count)) %>%
  group_by(actor) %>%
  mutate(percentage_actor = sum(count)) %>%
  mutate(percentage_right = sum(count)) %>%
  ungroup() %>%
  mutate(percentage_actor = (percentage_actor / sum_predictions) * 100) %>%
  mutate(percentage_right = (count / percentage_right) * 100) %>%
  filter(prediction == "rightly classified") %>%
  select(actor, percentage_right, percentage_actor) %>%
  pivot_longer(!actor, names_to = "category", values_to = "percentage") %>%
  # graph
  ggplot(aes(x = reorder(actor, percentage), y = percentage, fill = category)) +
  geom_bar(position = "dodge", stat = "identity") +
  coord_flip() +
  scale_y_continuous(
    expand = c(0, 0),
    position = "right",
    limits = c(0,61),
    breaks = seq(10,60,10),
    labels = c("10","20","30","40","50","60%")
  ) +
  scale_fill_manual(
    breaks = c("percentage_right", "percentage_actor"),
    values = c("#1b9e77", "#ffa600"),
    labels = c("Percentage of right Predictions if Speaker was predicted", "Percentage of total Predictions")
  ) +
  labs(
    title = "XGBoost with Number of Words as Tuneing Parameter:\nPredictions by Actor",
  ) +
    guides(fill=guide_legend(nrow=2,byrow=TRUE)) + 
  bar_chart_theme() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 5.5),
    legend.title = element_blank(),
    legend.key.height = unit(0.4, "line"),
    legend.key = element_blank(),
    legend.margin = margin(t = 0, unit = "cm")
  )

ggsave("./output/images/machine_learning/ml_xgb_tuned_actor.jpg", width = 4, height = 3)
```

## Conclusion

In summary, we have seen that it is hard to distinguish between the members of Critical Role using the subtitles. We did not achieve any satisfactory results with any of the method we used. Still, some caution is warranted. First, we used down-sampling to save computational resources. Using up-sampling, the results might improve. We could also give up on the idea of equal representation of the speakers altogether. If we cared about the predictions only, this approach is likely to achieve better results.

Second, we turned all categories into variables of zeros and ones. This approach is not ideal for using decision trees and might also influence its extensions. Thus, their predictions might improve by leaving the categories as one variable. Third, we could try to use the predictions from different models and combine them into an ensemble of methods. As we have seen, different models may do a better job of predicting certain speakers than others. Hence, combining their predictions could improve our predictions. If we had no computational constraints, we could also train a neural net. Thus, I encourage interested readers to experiment with the data themselves to find alternative approaches.

The code and the data set can be found on [GitHub](https://github.com/datengefluester/Critical-Role-NLP).

## Acknowledgements

-   [Critical Role](https://critrole.com/team/)

-   [Critical Role Transcript Team](https://crtranscript.tumblr.com/about)

## Licence:

This work is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License][cc-by-sa]
