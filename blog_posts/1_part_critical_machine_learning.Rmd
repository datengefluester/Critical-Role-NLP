---
title: "Critical Machine Learning: Prediciting Critical Role Actors from Text. Part 1"
always_allow_html: true
output:
  html_document: default
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_format = "all") })
---

```{r setup, include=FALSE}
# tell Rmarkdown to run in root folder of project
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

# where figures for rendering the document are saved
knitr::opts_chunk$set(
  fig.path = "markdown_figs/"
)


# packages
library(dplyr) # for data preparation
library(tidyr) # for data preparation
library(stringr) # for data preparation
library(tidytext) # for rearrange facet + bar charts
library(ggplot2) # for graphs
library(kableExtra) # for table
library(data.table) # for faster reading in data
```

```{r themes, include=FALSE}
# graph themes
source("./scripts/themes.R")
```

```{r data, echo = FALSE}
# ------ time and turns by actor
actor_words_time_percent <- read.csv('./data/descriptive_analysis/actor_words_time.csv')
actor_words_time_percent <- actor_words_time_percent %>%
  select(1, 4, 10) %>%
  pivot_longer(
    cols = c("words_percent","turns_percent"),
    names_to = "variable",
    values_to = "percent"
  ) %>%
  mutate(
    variable = str_replace(variable, "words_percent", "Words"),
    variable = str_replace(variable, "turns_percent", "Turns")
  )

# ------ example data
example <- read.csv('./data/descriptive_analysis/example.csv')
example_ml <- read.csv("./data/machine_learning/example_ml/example_ml.csv")
example_ml <- example_ml %>% 
  select(actor_guest,text,time_in_sec,words_per_minute,arc,segment,combat) %>%
  mutate(segment= str_to_title(segment)) %>% 
  rename(Actor = actor_guest,
         Text = text,
        `Time in Sec` = time_in_sec,
       `Words per Minute` =  words_per_minute,
       Arc = arc,
       Segment = segment,
       Combat = combat) 

# ------ Cross Validation Graph
nrows <- 10
cross_validation <- expand.grid(y = 1:nrows, x = 1:nrows)

cross_validation <- cross_validation %>% 
  mutate(train_test = "Train Data") %>% 
  mutate(train_test = replace(train_test, x==y, "Test Data")) %>% 
  mutate(train_test = as.factor(train_test))
rm(nrows)
```

[In the last post](https://datengefluester.de/critical-graphs-shenanigans-with-critical-role-season-1-subtitles/), we looked at the subtitles from season one of Critical Role. During our analysis, we saw that some words could potentially tell us which text is spoken by which actor. In this two-part series, we go on step further and examine whether we can indeed predict the speaker using more advanced methods. These more advanced methods are part of what is called machine learning, or commonly referred to as 'artificial intelligence'. Loosely speaking, machine learning allows computers to learn patterns in the data to predict an outcome. One common task for machine learning is predicting the 'label' of a text (in our case the actor, who speaks it). This is used, for example, by your email provider when it decides whether an email is spam or not. The goal of this series is to see whether we can also use machine learning to match the subtitles to actors of Critical Role. In the first post, we
cover the preparations and considerations needed before applying the machine learning algorithms. In the second post, we will look at the different techniques available and compare their results against each other.

In a previous post we saw that Critical Role as a series contains 431 hours of game play over 114 episodes. Its subtitles look something like this:

> Laura: 'Can we fly the carpet forward?'
>
> Matt: 'You're currently not in control of the
> carpet. Tiberius is.'

In the last post, we called each of these two texts a 'turn'. However, in the machine learning context, they are called 'classified samples'. We can use these samples to build our model to predict the speaker. In total, our data contains 280,000 classified samples, which we can use to train our algorithms.

Unfortunately, we cannot use all the data to train our machine learning algorithms. Instead, we must split it into two separate data sets. Why is that? Like in a school, for us to quantify, which students learned the most in class, we need to test them in an exam. This exam should contain similar but unknown tasks to ones studied in class, so we can judge how well the students understood the underlying concepts. If students knew all questions
beforehand, their most efficient approach would be to learn all the answers by heart. While this helps them to ace the test, it leaves them clueless when you get asked a slightly different question on the same topic. In other words, their knowledge does not generalize well. The same is true for algorithms. If they know, on which texts they're tested on, they will only focus on those and ignore everything else. As a result, they would be able predict the right speaker given the exam texts but fail miserably at all other texts. Therefore, we want to have a final exam which contains texts, which the algorithms have not seen before. This allows us to judge how well the predictions of the different models generalize. It also enables us to compare the algorithms against each other. In machine learning jargon, the final test is referred to as the 'test data set'.

The remaining data is called 'training data set'. As the name suggests, we can use it to train our algorithms. On a meta level, we can think of the different algorithms as machines made of many gearwheels. If we feed the machine a text, it will process the text through its wheels and produce a prediction for who it thinks said the text. As different configurations of these wheels lead to different predictions by the machine, we need a way to tell which configuration works best. We accomplish this by mimicking mock exams. These mock exams help us and the algorithms to find their best configurations. If it assigned the wrong speaker to a text, it could change its wheels such that it improves its performance.

So how do we mimic mock exams for our algorithms? In the first step, we split our training data into multiple random equally sized sub-sets. In the second step, we train the models multiple times, with each of the set functioning as a testing data set (the mock exam) at different times. Given that both the final test data and the sets are random draws from the full data set, our mock exams should on average resemble our final test. In machine learning jargon, each set is called a 'fold' and the process of tuning the algorithms is called 'cross-validation'. A common practice is to subset the data ten times to train the algorithm (10-fold cross-validation). This means that we use 90 percent of the data to train our algorithm and test it on 10 percent of the data. We repeat the process 10 times to find the best specification. The image below pictures the process. Fortunately, unlike a student, a computer algorithm does not have to decide between socializing and studying. Therefore, we can let it take multiple mock exams simultaneously.

```{r cross_validation, echo = FALSE}
ggplot(cross_validation, aes(x = x, y = y, fill = train_test)) + 
  geom_tile(color = "#3B3B3B", size = 0.1, alpha = 0.7) +
  scale_x_continuous(expand = c(0, 0),
                     breaks = c(1,10),
                     labels = c("1"="First 10%",
                                "10"="Last 10%")) +
  scale_y_continuous(expand = c(0, 0), trans = 'reverse',
                     breaks = c(seq(0, 10, 1)),
                     labels = c("0" ="",
                                "1" = "Fold 1", 
                                "2" = "Fold 2",
                                "3" = "Fold 3",
                                "4" = "Fold 4",
                                "5" = "Fold 5",
                                "6" = "Fold 6",
                                "7" = "Fold 7",
                                "8" = "Fold 8",
                                "9" = "Fold 9",
                                "10" = "Fold 10")) +
   scale_fill_manual(
    values = c("#ffa600","#1b9e77")
  ) +
  labs(title="10-Fold Cross Validation",
       subtitle="One Block represents Ten Percent of the Training Data") +
  bar_chart_theme() +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 5.5),
    legend.title = element_blank(),
    legend.key.height = unit(0.4, "line"),
    legend.key = element_blank(),
    legend.margin = margin(t = 0, unit = "cm"),
    axis.title.x=element_blank(),
    axis.ticks.x=element_blank(),
    axis.ticks.y=element_blank(),
    panel.grid.major.x = element_blank() ,
    )
# save
ggsave("./output/images/machine_learning/ml_cross_validation.jpg", width = 4, height = 3)
```

As we have seen in the last post, not every actor was present for every episode. While Orion only participated in 24 episodes, Matt was there for all of them. Similarly, Matt's role as the dungeon master means that he is speaking more compared to other actors. This means that we have more classified samples for him than for any other cast member. We need to account for this imbalance when judging our algorithms. As 28 percent of our texts belong to Matt, simply guessing him for every text means we're correct in more than
a quarter of cases. Therefore, every algorithm must beat this benchmark to be considered an improvement over simply predicting Matt for all texts.

As a result of the imbalances in the data our training and testing data sets will also contain more texts from Matt. This means that our algorithms are likely to learn more features of Matt's texts, as this results in better predictions. In
turn, our models would not properly learn how to predict Ashley, Orion, or Guests. As our goal is to differentiate between the different actors, this is not ideal. So how would we prevent the algorithms from focusing on Matt?

```{r time_vs_words, echo = FALSE}
actor_words_time_percent %>%
  filter(variable != "Words") %>% 
  mutate(
    actor_guest = reorder_within(actor_guest, -percent, variable)
  ) %>%
  ggplot(aes(x = reorder(as.factor(actor_guest), percent), y = percent, fill = variable)) +
  geom_bar(stat = "identity", fill = "#009E73") +
  coord_flip() +
  scale_y_continuous(expand = c(0, 0),
                     breaks =c(10,20,30),
                     limit = c(0,31)) +
  scale_x_reordered() +
  labs(
    title = "Samples per Actor",
    subtitle = "In percent of the total number of samples.",
    caption = "Source: own calculations, Critical-Role-Subtitles",
  ) +
  bar_chart_theme() +
  theme(
    strip.background = element_rect(fill = "#F0F0F0"),
    panel.grid.major.x = element_line(size = rel(0.4), color = "#656565"),
    axis.line.y = element_line(size = rel(0.4), color = "black"),
    axis.text.x = element_text(color = "black", vjust = 5, size = rel(0.5), alpha(2)),
    axis.text.y = element_text(hjust = 1, color = "black", size = rel(0.75), margin = margin(r = -2)),
    strip.text = element_text(size = 5, face = "bold"),
  )

# save graph
ggsave("./output/images/machine_learning/ml_time_words_per_actor.jpg", width = 4, height = 3)
```

One approach to tackle the imbalance in the data is called 'down-sampling'. The idea is to randomly leave out some texts by actors, who speak more than the others, until everybody has the same number of texts. This will help us to better identify text spoken by speakers, who speak less. The downside of this approach is that it gives speakers, who speak little, more weight than they should have. Hence, if our goal would only be to achieve the best prediction, we would not down-sample. However, using down-sampling prevents our models from focusing on mainly trying to predict Matt.

We could also do the opposite and increase the number of samples for the actor, who speak less (a process is called 'up-sampling'). To do so, we would duplicate texts for every speaker, but Matt until every speaker has the same number of texts. Given the imbalance of the data, this would significantly increase the time required to train the algorithms. However, it likely to result in better model performance. Still, as some models already take quite some time to train, we down-sample to save time (and money, as computing time is expensive).

An additional benefit of down-sampling and up-sampling is that it allows us to use accuracy as our metric to compare different algorithms. Accuracy measures the number of right predictions relative to all predictions. Basically, it tells us the percentage of texts our algorithms assigned the right speaker to. Using accuracy as our metric gives us the easiest and more illustrative way of comparing the different models.

As we have seen in our previous analysis, it sometimes happens that actors say the same thing simultaneously. If we considered each of these cases as distinct speakers (for example,Â 'Laura and Travis' as one speaker), we would increase the number of speakers we would need to predict to 110. Since some of these only happen once in the entire series, we omit these texts and stick to our ten speakers (the cast member and 'guests').

Putting everything together (splitting into training and testing data sets, down-sampling and dropping texts said by multiple actors), we end up with 34,500 samples in our training data set, which we can use to train our machine learning models. We then compare the different models on the test data set, which contains 66,000 texts. The fact that the test data is larger than the training data is not ideal. Yet, to get things started we will stick with the setup for now. However, I encourage interested readers to repeat the analysis using a different approach.

Until now, we have prepared the data such that we can compare different models, and they can differentiate between the different speakers. Now, we must decide on how to feed the data into our machine learning algorithms. Recall, that our raw text data looks as follows:

```{r example, echo = FALSE}
example %>%
  select(-Segment) %>% 
  kbl() %>%
  kable_styling()
```

We can include some additional information to ultimately turn the data into something like this:

```{r example_ml, echo = FALSE}
example_ml %>%
  kbl() %>%
  kable_styling()
```

Note that we dropped the episode number from the data. We do so, to not have our algorithms learn on episode specific dialogue. However, we keep the arc to allow for some general changes in the story to be represented in the data.

Next, we need to turn the text into a nicer format. To do so, we must transform it into separate, smaller pieces called tokens. The tokens can be single words or characters, sentences or combinations of words called n-grams. For example, the text 'I love you' can be separated into 2-grams as follows: 'I love' and 'love you'.

Yet, most of the text is not helpful to differentiate between the players. For example, all players announce their action using some form of 'I' (I will..., I am going to... etc.). Likewise, many words, like 'and' or 'the', do not add much unique meaning to aid us in our task. To decide which words, we should include in our analysis, we can calculate the 'term frequency inverse document frequency' (tfidf). The frequency calculates how often a token is used, while adjusting for the number of texts it is used in. Using this measure, we can quantify, which tokens are most helpful for predicting the speaker.

In theory, we could use all the tokenized words and up to whole-texts-n-grams to predict a speaker. However, doing so would significantly increase the time to train our models. Instead, we could let the algorithm decide how many tokens it would like to include for its predicting (thus treating the number of words or n-grams as another moving part in the machine). Again, this would also lengthen the training process. To achieve a balance between good prediction and a reasonable time to train, we restrict our analysis to include the 300 most predictive words, 2-grams, and 3-grams. Like many things in this analysis, this decision follows no hard coded rules but instead tries to balance performance and time needed to obtain the results. In combination with extracting some text features, like the number of words in a text and the speed of talking, this gives a total of 318 variables to predict the speakers.

In summary, we discussed the general approach and the preparations necessary to use machine learning algorithms to predict the speaker from the text. We turned the text into the 318 features, which we can use for the predictions. Splitting the text into training and test data sets and adjusting for the imbalance in the data gives us more 34,000 texts, which we can use to train machine learning algorithms. In the next post we will see whether we
can indeed predict the speaker using some commonly used machine learning algorithms.
